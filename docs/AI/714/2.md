# 2026-01-24

写一些714框架的读后感！

## `autograd.py`

### value类

（有计算图的很多基本操作，比如创建节点 const）

```python
@classmethod
def make_const(cls, data, *, requires_grad=False):
    value = cls.__new__(cls)
    value._init(
        None,
        [],
        cached_data=data,
        requires_grad=requires_grad,
    )
    return value

@classmethod
def make_from_op(cls, op: Op, inputs: List["Value"]):
    value = cls.__new__(cls)
    value._init(op, inputs)

    if not LAZY_MODE:
        if not value.requires_grad:
            return value.detach()
        value.realize_cached_data()
    return value
```

其中`realize_cached_data()`这个函数就是过计算图给结果的，可以看Trae AI的解释：

**执行流程**
1. 检查缓存 ：如果 cached_data 不为 None （已计算过），直接返回缓存的结果，避免重复计算
2. 递归计算输入 ：对所有输入值递归调用 realize_cached_data() ，确保所有依赖项都已计算
3. 执行计算 ：调用当前操作（ self.op ）的 compute 方法，传入计算后的输入值
4. 缓存结果 ：将计算结果存储到 self.cached_data 中
5. 返回结果 ：返回计算后的数据

const是没有`op`的，而其他是有的，需要分开来make。 但这两个函数在Tensor里又重写了

### Tensor类

Tensor类是继承了Value类的，前面先做了一些传递dtype,device等的操作。后面重写了两个 `static_method`:

```python
@staticmethod
def make_from_op(op: Op, inputs: List["Value"]):
    tensor = Tensor.__new__(Tensor)
    tensor._init(op, inputs)
    if not LAZY_MODE:
        if not tensor.requires_grad:
            return tensor.detach()
        tensor.realize_cached_data()
    return tensor

@staticmethod
def make_const(data, requires_grad=False):
    tensor = Tensor.__new__(Tensor)
    tensor._init(
        None,
        [],
        cached_data=data
        if not isinstance(data, Tensor)
        else data.realize_cached_data(),
        requires_grad=requires_grad,
    )
    return tensor
```

这里要理解的话主要就是限制类型一定会是Tensor。可能后面Tensor会出现一些子类，但要保证这些子类make的一定要是Tensor自己

### 运算符重载

并不难，但需要注意有这么一回事（）

```python
def __add__(self, other):
    if isinstance(other, Tensor):
        return needle.ops.EWiseAdd()(self, other)
    else:
        return needle.ops.AddScalar(other)(self)
```

## `nn_basic.py`

### Module类

- `training`: 布尔值，表示当前是否处于训练模式
- `parameters()`: 返回模型的所有可训练参数
- `forward(*args)`: 定义前向传播逻辑
- `eval()`: 设置为评估模式
- `train()`: 设置为训练模式
- `__call__(*args)`: 允许像函数一样调用模型

### Parameter 类

```python
class Parameter(Tensor):
    """A special kind of tensor that represents parameters."""


def _unpack_params(value: object) -> list[Tensor]:
    if isinstance(value, Parameter):
        return [value]
    elif isinstance(value, Module):
        return value.parameters()
    elif isinstance(value, dict):
        params = []
        for k, v in value.items():
            params += _unpack_params(v)
        return params
    elif isinstance(value, (list, tuple)):
        params = []
        for v in value:
            params += _unpack_params(v)
        return params
    else:
        return []
```

这个类倒没什么，主要是这个函数（在`module`里被定义为`parameter()`），通过递归的方式收集所有的参数，看这个例子比较好懂

```python
import needle as ndl
import needle.nn as nn
import needle.optim as optim

# 定义一个简单的神经网络模型
class SimpleMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        # 定义网络层
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# 创建模型实例
model = SimpleMLP(input_dim=784, hidden_dim=128, output_dim=10)

# 1. 使用 parameters() 方法获取所有参数
print("=== 模型参数 ===")
params = model.parameters()
print(f"参数数量: {len(params)}")
print(f"第一个参数形状: {params[0].shape}")  # fc1 的权重
print(f"第二个参数形状: {params[1].shape}")  # fc1 的偏置
print(f"第三个参数形状: {params[2].shape}")  # fc2 的权重
print(f"第四个参数形状: {params[3].shape}")  # fc2 的偏置

# 2. 将参数传递给优化器
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 3. 在训练循环中使用
def train_step(model, optimizer, x, y):
    # 前向传播
    logits = model(x)
    loss = nn.softmax_loss(logits, y)
    
    # 反向传播
    loss.backward()
    
    # 更新参数
    optimizer.step()
    
    # 清零梯度
    optimizer.zero_grad()
    
    return loss.data

# 模拟训练数据
x = ndl.Tensor(ndl.ops.randn(32, 784))  # 32个样本，每个样本784个特征
y = ndl.Tensor(ndl.ops.randint(0, 10, (32,)))  # 32个标签

# 执行训练步骤
loss = train_step(model, optimizer, x, y)
print(f"\n训练步骤损失: {loss}")
```

## 针对Question5读的框架和思路

### ResidualBlock 和 MLPResNet

这两道题纯纯搭积木，用Sequential连起来就可以了，还挺好做的。

```python
def MLPResNet(
    dim,
    hidden_dim=100,
    num_blocks=3,
    num_classes=10,
    norm=nn.BatchNorm1d,
    drop_prob=0.1,
):
    ### BEGIN YOUR SOLUTION
    return nn.Sequential(
        nn.Linear(dim,hidden_dim),
        nn.ReLU(),
        *[ResidualBlock(dim=hidden_dim, hidden_dim=hidden_dim//2, norm=norm, drop_prob=drop_prob) for _ in range(num_blocks)],
        nn.Linear(hidden_dim//2,num_classes)
    )
    ### END YOUR SOLUTION
```

return的是`nn.Sequential`，是一个`module`

## `epoch`

### Parameters

先研究一下`Parameters`

### DataLoader

1. `needle.data.DataLoader`: 

啊，我以前怎么抄的代码，怪不得我没有印象

```python
class DataLoader:
    r"""
    Data loader. Combines a dataset and a sampler, and provides an iterable over
    the given dataset.
    Args:
        dataset (Dataset): dataset from which to load the data.
        batch_size (int, optional): how many samples per batch to load
            (default: ``1``).
        shuffle (bool, optional): set to ``True`` to have the data reshuffled
            at every epoch (default: ``False``).
     """
    dataset: Dataset
    batch_size: Optional[int]

    def __init__(
        self,
        dataset: Dataset,
        batch_size: Optional[int] = 1,
        shuffle: bool = False,
    ):

        self.dataset = dataset
        self.shuffle = shuffle
        self.batch_size = batch_size
        if not self.shuffle:
            self.ordering = np.array_split(np.arange(len(dataset)), 
                                           range(batch_size, len(dataset), batch_size))
    def __iter__(self):
        ### BEGIN YOUR SOLUTION
        self.index=0
        if self.shuffle:
            self.ordering = np.array(np.arange(len(self.dataset)))
            np.random.shuffle(self.ordering)
            self.ordering = np.array_split(self.ordering,range(self.batch_size, len(self.dataset), self.batch_size))
            
        ### END YOUR SOLUTION
        return self

    def __next__(self):
        ### BEGIN YOUR SOLUTION
        if self.index>=len(self.ordering):
            raise StopIteration
        else:
            # current_data = self.dataset[self.ordering[self.index]]
            # 唉，抄答案有罪，但是懒得翻框架了。。。
            current_data = [Tensor.make_const(x) for x in self.dataset[self.ordering[self.index]]]
            self.index += 1
            return current_data
        ### END YOUR SOLUTION
```

这里DataLoader应该是迭代器，这里还是复习一下`init, iter, next`三个方法：

```python
class MyCountdown:
    def __init__(self, start):
        # 初始化：设置从哪里开始倒数
        self.current = start

    def __iter__(self):
        # 必须返回对象本身，这样它才能用在 for 循环里
        print("start!")
        return self

    def __next__(self):
        # 核心逻辑：
        if self.current > 0:
            num = self.current
            self.current -= 1
            return num  # 返回当前数字
        else:
            # 如果数完了，必须抛出 StopIteration 异常
            # 否则 for 循环永远不会停止
            raise StopIteration

# --- 如何使用这个类 ---

# 1. 在 for 循环中使用 (最常用的方式)
print("使用 for 循环:")
counter = MyCountdown(3)
for num in counter:
    print(num)

# 2. 手动调用 (理解原理)
print("\n手动调用:")
counter2 = MyCountdown(2)
it = iter(counter2) # 实际上调用了 __iter__
print(next(it))     # 实际上调用了 __next__，得到 2
print(next(it))     # 得到 1
# print(next(it))   # 如果再调一次，会报错 StopIteration
```

回到我们之前的迭代器，我们主要看一下`__next__`里return的啥东西：`current_data`，而`current_data = [Tensor.make_const(x) for x in self.dataset[self.ordering[self.index]]]`，应该是按照self.ordering排列了dataset里的顺序


### backward

刚刚碰到的小问题主要是突然不知道backward之后会有什么结果，翻了一下是batch_loss作为一个Tensor调用了backward()。

```python
def backward(self, out_grad=None):
    out_grad = (
        out_grad
        if out_grad
        else init.ones(*self.shape, dtype=self.dtype, device=self.device)
    )
    compute_gradient_of_variables(self, out_grad)
```

读这个感觉就是在计算每个节点的`out_grad`，梯度（计算图原理）