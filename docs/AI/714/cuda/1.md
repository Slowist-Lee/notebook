# 编程模型

## GPU - 分层组织结构

![|500](Pasted%20image%2020260126225546.png)

不同块内线程不能相互影响，他们是物理隔离的

依靠下面两个内置结构体确定线程标号：

- blockIdx（线程块在线程网格内的位置索引）
- threadIdx（线程在线程块内的位置索引）

注意这里的Idx是index的缩写，这两个内置结构体基于 uint3 定义，包含三个无符号整数的结构，通过三个字段来指定：

- blockIdx.x
- blockIdx.y
- blockIdx.z
- threadIdx.x
- threadIdx.y
- threadIdx.z

上面这两个是坐标，当然我们要有同样对应的两个结构体来保存其范围，也就是blockIdx中三个字段的范围threadIdx中三个字段的范围：

- blockDim
- gridDim

他们是dim3类型(基于uint3定义的数据结构)的变量，也包含三个字段x,y,z.

- blockDim.x
- blockDim.y
- blockDim.z

网格和块的维度一般是二维和三维的，也就是说一个网格通常被分成二维的块，而每个块常被分成三维的线程。

## 核函数

启动核函数，通过的以下的ANSI C 扩展出的CUDA C指令：  

```cpp
kernel_name<<<grid,block>>>(argument list);
```

其标准C的原型就是C语言函数调用  

```python
function_name(argument list);
```

## 第一个例子

```c++
// 包含CUDA运行时API的头文件
#include <cuda_runtime.h>

// 在GPU上执行的核函数(Kernel)
__global__ void vector_add(const float* A, const float* B, float* C, int N) {
    // 计算当前线程的全局唯一ID
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    // 边界检查，防止越界访问
    if (i < N) {
        // 执行向量加法
        C[i] = A[i] + B[i];
    }
}

// 在CPU上执行的函数，用于启动GPU核函数
// A, B, C 是指向GPU内存的设备指针
extern "C" void solve(const float* A, const float* B, float* C, int N) {
    // 定义每个线程块中的线程数量
    int threadsPerBlock = 256;
    // 计算需要的线程块数量
    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
    
    // 启动核函数
    vector_add<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, N);
    
    // 等待GPU任务完成
    cudaDeviceSynchronize();
}
```

### 1. 代码解析

#### 1.1 头文件

```c++
#include <cuda_runtime.h>
```

这行代码包含了CUDA运行时API的头文件。这个文件提供了在主机（CPU）端管理和控制GPU设备所需的所有函数，例如内存分配 (`cudaMalloc`)、数据传输 (`cudaMemcpy`)、核函数启动以及设备同步 (`cudaDeviceSynchronize`) 等。

#### 1.2 核函数 (Kernel Function)

```c++
__global__ void vector_add(const float* A, const float* B, float* C, int N)
```

*   `__global__`: 这是一个CUDA关键字，它告诉编译器这个函数（称为“核函数”或“Kernel”）是在**GPU设备**上执行的，并且可以从**CPU主机**端调用。
*   `void vector_add(...)`: 核函数的返回类型必须是 `void`。它的参数 `A`, `B`, `C` 是指向GPU内存的指针，`N` 是向量的长度。

##### **线程索引计算**

```c++
int i = blockIdx.x * blockDim.x + threadIdx.x;
```

这是CUDA编程中最核心和最关键的一行代码。为了理解它，我们需要先了解CUDA的执行模型。

> [!IMPORTANT] CUDA执行模型：Grid、Block、Thread
> CUDA通过一种层次化的方式来组织线程，以便进行高效的并行计算：
> *   **线程 (Thread)**: 最基本的执行单元。
> *   **线程块 (Block)**: 一组线程的集合。同一个块内的线程可以相互协作，例如通过共享内存（Shared Memory）和同步（Synchronization）来交换数据。
> *   **网格 (Grid)**: 一组线程块的集合。一个核函数的单次启动会创建一个网格。
>
> 它们都是三维的，但在这个一维向量加法的例子中，我们只使用了一维（`.x` 维度）。
>
> `blockIdx.x`、`blockDim.x` 和 `threadIdx.x` 是CUDA提供的内置变量：
> *   `threadIdx.x`: 当前线程在**其所在线程块内**的索引（ID）。
> *   `blockDim.x`: 当前线程块的维度，即**一个块中有多少个线程**。在这个例子中，它的值是256。
> *   `blockIdx.x`: 当前线程块在**整个网格内**的索引（ID）。
>
> 通过 `blockIdx.x * blockDim.x + threadIdx.x` 这个公式，我们可以为网格中的每一个线程计算出一个全局唯一的ID。这个ID正好可以用来对应要处理的向量 `A`, `B`, `C` 中的元素索引。

**【建议在此处补图】**
一张图，展示一个一维的Grid，其中包含多个Block，每个Block又包含多个Thread。图中标示出 `blockIdx.x` 从0开始递增，以及每个Block内部的 `threadIdx.x` 也是从0开始递增。并用一个箭头指向某个特定的线程，演示如何通过公式计算出它的全局索引 `i`。

##### **边界检查与核心计算**

```c++
if (i < N) {
    C[i] = A[i] + B[i];
}
```

*   **边界检查 (`if (i < N)`)**: 我们启动的线程总数（`blocksPerGrid * threadsPerBlock`）通常会略大于向量的实际长度 `N`。这是因为 `N` 可能无法被 `threadsPerBlock` 整除。因此，必须进行这个检查，确保只有索引 `i` 小于 `N` 的线程才执行访存和计算操作，否则会导致数组越界。
*   **核心计算 (`C[i] = A[i] + B[i]`)**: 每个线程负责处理一个元素的加法。由于成千上万的线程在GPU上是并行执行的，所以整个向量的加法可以被极大地加速。

#### 1.3 主机函数 (Host Function)

```c++
extern "C" void solve(const float* A, const float* B, float* C, int N)
```
这是一个在CPU上执行的普通函数。`extern "C"` 确保了函数名在编译后不会被C++的命名修饰（name mangling）机制改变，这在需要从其他语言（如Python）调用此函数时很有用。

##### **执行配置**

```c++
int threadsPerBlock = 256;
int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
```

这两行代码设置了核函数的执行配置（Execution Configuration）。
*   `threadsPerBlock`: 定义了每个线程块包含256个线程。这个值通常选择为32的倍数，因为GPU中32个线程组成一个“线程束”（Warp），是基本的调度单元。256是一个常见且高效的选择。
*   `blocksPerGrid`: 计算完成所有 `N` 个元素的计算需要多少个线程块。`(N + threadsPerBlock - 1) / threadsPerBlock` 是一种标准的向上取整的整数除法技巧。例如，如果 `N=513`, `threadsPerBlock=256`，那么 `(513 + 255) / 256 = 768 / 256 = 3`，需要3个块才能覆盖所有元素。如果用 `N / threadsPerBlock` 则会得到2，导致最后一个元素被遗漏。

##### **核函数启动**

```c++
vector_add<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, N);
```

这是从CPU启动GPU核函数的语法。
*   `<<<...>>>`: 这是CUDA特有的核函数启动语法。
*   `blocksPerGrid`: 第一个参数，指定了本次启动的网格中包含的线程块数量。
*   `threadsPerBlock`: 第二个参数，指定了每个线程块中包含的线程数量。
*   `(...)`: 括号内是传递给核函数 `vector_add` 的参数。

这行代码执行后，控制权会**立即返回**到CPU，CPU可以继续执行其他任务，而GPU则开始异步地执行 `vector_add` 核函数。

##### **设备同步**

```c++
cudaDeviceSynchronize();
```
由于核函数是异步执行的，CPU代码不会等待GPU完成。如果在 `solve` 函数返回后，CPU需要立即使用GPU的计算结果（存储在 `C` 中），那么就必须调用这个函数。`cudaDeviceSynchronize()` 会**阻塞CPU**，直到GPU上所有先前启动的任务（包括 `vector_add` 核函数）全部执行完毕。

### 2. 总结

这个简单的例子完整地展示了一次CUDA编程的基本流程：

1.  **定义核函数 (`__global__`)**: 编写将在GPU上并行执行的代码逻辑，并使用线程ID来区分每个线程的任务。
2.  **设置执行配置**: 在CPU端根据问题规模（如向量长度 `N`）计算出需要的线程块和线程数量。
3.  **启动核函数 (`<<<...>>>`)**: 从CPU向GPU下达异步执行指令。
4.  **同步 (`cudaDeviceSynchronize`)**: 在需要等待GPU结果的地方，阻塞CPU，确保GPU任务已完成。

通过将一个大的循环（向量加法）拆解成成千上万个独立的、可以并行执行的小任务，并交由GPU的众多核心来处理，CUDA实现了远超CPU串行执行的计算性能。