# NN Library Implementation

好的，没问题！这是一份根据你提供的字幕和 Jupyter Notebook精心编写的、面向新手的中文学习笔记。笔记采用了 Markdown 格式，并合理使用了 Obsidian Callout 语法来突出重点和示例，希望能帮助你轻松入门。

---

# 机器学习系统：神经网络库的实现 (CMU 10-714) - 讲座 8 学习笔记

> 欢迎来到 **深度学习系统：算法与实现** 的第八讲！
>
> 在上一讲中，我们讨论了构建一个神经网络库所需的各个组件。今天，我们将通过一个 IPython Notebook，亲手实现这些组件，深入理解神经网络的实现细节。
>
> 这是一节实践性很强的课，所以我们将告别幻灯片，直接在代码世界里探索。

---

## Part 1: 基础准备与环境设置

在开始编码之前，我们需要先准备好我们的工作环境和代码库。

### 1.1 克隆代码库

首先，我们需要将课程提供的代码库克隆到本地。如果你是第一次运行，需要执行 `git clone` 命令。如果已经克隆过了，为了保留你自己的修改，可以注释掉这行命令，避免重复下载。

```python
# 挂载 Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 进入工作目录，如果不存在则创建
%cd /content/drive/MyDrive/
!mkdir -p 10714f25
%cd /content/drive/MyDrive/10714f25

# 第一次运行时取消注释下面这行
# !git clone https://github.com/dlsyscourse/lecture8

# 创建符号链接并将目录切换到 needle
!ln -s /content/drive/MyDrive/10714f25/lecture8 /content/needle
%cd /content/needle
```

### 1.2 设置 Python 环境

为了让我们的 Python 环境能够找到 `needle` 库，需要将它的路径添加到环境变量中。

```python
%set_env PYTHONPATH /content/needle/python:/env/python
import sys
sys.path.append("/content/needle/python")
```

> [!TIP] 重要提示
> 本次讲座的代码依赖于之前作业中实现的自动微分功能。你需要将你自己完成的 `autograd` 相关代码复制到 `needle` 项目的相应文件中，才能顺利运行接下来的演示。

---

## Part 2: Needle 框架温故知新

在深入构建神经网络库之前，我们先来回顾和探讨一下 `needle` 框架中一些关键且非常实用的概念。

### 2.1 张量 (Tensor) 的数据更新

在训练神经网络时，最核心的操作之一就是**更新权重**。我们通过计算梯度，然后根据梯度下降法来调整权重的值。一个常见的梯度更新公式如下：

$W_{new} = W_{old} - \eta \cdot \nabla L$

这里，$W$ 是权重，$\eta$ 是学习率，$\nabla L$ 是损失函数对权重的梯度。

让我们看看一个看似正确的实现方式，以及它隐藏的问题。

```python
import needle as ndl

# 初始化权重 w 和梯度 grad
w = ndl.Tensor([1, 2, 3], dtype="float32")
grad = ndl.Tensor([1, 1, 1], dtype="float32")
lr = 0.1

# 模拟5次梯度更新
for i in range(5):
    # w = w - lr * grad  (假设我们已经实现了减法)
    # 因为 lecture 8 的代码库没有减法，我们用加一个负数来代替
    w = w + (-lr) * grad

print(w)
```

> [!DANGER] 陷阱：不断增长的计算图
>
> 上面的代码虽然能得到正确的数值结果，但存在一个巨大的性能隐患。
>
> 默认情况下，`ndl.Tensor` 的 `requires_grad` 属性为 `True`，这意味着 `needle` 会追踪所有与这个张量相关的计算，并构建一个**计算图 (Computational Graph)**，以便进行自动微分。
>
> 在循环更新 `w` 的过程中，每一步操作 (`+`, `*`) 都会在计算图上增加一个新的节点。循环结束后，`w` 张量不仅包含了最终的数值，还拖着一个记录了所有历史更新步骤的庞大计算图。
>
> ```python
> # 检查 w 的操作历史
> print(w.op)
> # >> <needle.ops.ops_mathematic.EWiseAdd object at ...>
> print(w.inputs[0].op)
> # >> <needle.ops.ops_mathematic.EWiseAdd object at ...>
> print(w.inputs[0].inputs[0].op)
> # >> <needle.ops.ops_mathematic.EWiseAdd object at ...>
> ```
>
> 你可以看到，这是一个长长的操作链！这会导致两个严重问题：
> 1.  **内存泄漏**：计算图会持有对所有中间结果的引用，导致内存无法被释放。随着训练的进行，内存占用会线性增长，最终导致程序崩溃。
> 2.  **性能下降**：构建和维护这个不必要的计算图会消耗额外的计算资源。
>
> 这在 PyTorch 等命令式自动微分框架中是一个非常常见的 bug。

### 2.2 正确的更新方式：`.data` 与 `detach()`

那么，如何避免上述问题呢？关键在于，**权重更新这个步骤本身是不需要计算梯度的**。我们只想简单地修改权重的值，而不想让这个修改操作成为计算图的一部分。

`needle` 提供了 `.data` 属性（或等效的 `.detach()` 方法）来解决这个问题。它会创建一个新的 Tensor，这个新的 Tensor 与原始 Tensor **共享底层的数据内存**，但是**不包含任何计算图信息** (它的 `requires_grad` 为 `False`)。

> [!NOTE] `.data` 的工作原理
>
> 当你调用 `w.data` 时，`needle` 内部会：
> 1.  创建一个新的 `Tensor` 对象。
> 2.  这个新 `Tensor` 共享 `w` 的底层数据存储，所以没有发生数据拷贝，效率很高。
> 3.  这个新 `Tensor` 的 `op` (操作) 和 `inputs` (输入) 都是空的，`requires_grad` 被设置为 `False`。
> 4.  这样就切断了与历史计算图的连接，我们称之为创建了一个**分离的 (detached)** 张量。

现在，我们来看正确的更新方式：

```python
# 重新初始化 w
w = ndl.Tensor([1, 2, 3], dtype="float32")
grad = ndl.Tensor([1, 1, 1], dtype="float32")
lr = 0.1

# 正确的更新循环
for i in range(5):
    # 1. 获取 w 和 grad 的数据，进行无图计算
    new_w_data = w.data + (-lr) * grad.data
    # 2. 将计算结果直接赋值回 w.data，实现原地更新
    w.data = new_w_data

print(w)
# >> needle.Tensor([0.5 1.5 2.5])
print(w.op)
# >> None (没有计算图历史！)
```

> [!SUCCESS] 最佳实践：原地更新 (In-place Update)
>
> 通过 `w.data = ...` 的方式，我们直接修改了 `w` 张量底层的数据，而不是创建一个全新的 `w` 变量。这是一种**原地更新**。
>
> 这种方式非常重要，因为在后续构建的神经网络模型中，优化器会持有对模型参数（比如 `w`）的引用。如果我们在更新时创建了一个新的 `w` 变量，那么优化器持有的旧引用就不会得到更新。而原地更新确保了所有持有 `w` 引用的地方都能看到最新的值。

### 2.3 数值稳定性问题 (Numerical Stability)

计算机使用浮点数来表示实数，但浮点数的精度是有限的。这会导致在进行数值计算时，尤其是在深度学习中，出现一些意想不到的问题，比如上溢 (overflow) 或下溢 (underflow)。

一个经典的例子就是 `Softmax` 函数的计算。

$z_i = \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{k} e^{x_k}}$

> [!EXAMPLE] 例：一个不稳定的 Softmax 实现
>
> 让我们用一个简单的 NumPy 实现来看看问题出在哪里。
>
> ```python
> import numpy as np
>
> def softmax_naive(x):
>     z = np.exp(x)
>     return z / np.sum(z)
>
> # 当输入值很大时
> x = np.array([100, 100, 101], dtype="float32")
> print(softmax_naive(x))
> # >> [nan nan nan] (出现了非数值 Not a Number)
> ```
>
> **问题分析**：当 `x` 中的数值很大时（比如 100），`np.exp(x)` 的结果会超出浮点数能表示的最大范围，导致**上溢 (overflow)**，变成了 `inf` (无穷大)。最终 `inf / inf` 的结果就是 `nan`。

> [!TIP] 解决方案：Log-Sum-Exp 技巧
>
> `Softmax` 函数有一个非常好的性质：分子分母同时乘以一个常数，结果不变。利用这个性质，我们可以对输入进行平移，避免上溢。
>
> $\text{softmax}(x_i) = \frac{e^{x_i} \cdot C}{\sum_{k} e^{x_k} \cdot C} = \frac{e^{x_i + \log C}}{\sum_{k} e^{x_k + \log C}} = \frac{e^{x_i - c}}{\sum_{k} e^{x_k - c}}$
>
> (令 $c = -\log C$)
>
> 一个绝佳的选择是令 $c = \max(x_i)$。这样，`x_i - c` 的最大值就变成了 0，所有 `exp` 的输入都不会是大的正数，从而避免了上溢问题。
>
> ```python
> def softmax_stable(x):
>     # 减去最大值，进行数值稳定化
>     x = x - np.max(x)
>     z = np.exp(x)
>     return z / np.sum(z)
>
> # 使用稳定版 Softmax
> x = np.array([1000, 10000, 100], dtype="float32")
> print(softmax_stable(x))
> # >> [0. 1. 0.] (结果正确且稳定)
> ```
>
> 这个技巧被称为 **Log-Sum-Exp trick**，在实现 `Softmax`、`LogSoftmax` 等函数时至关重要，你将会在作业中用到它。

---

## Part 3: 设计一个神经网络库

有了 `needle` 的基础，我们现在可以开始设计一个更高级的神经网络库，就像 PyTorch 中的 `torch.nn` 一样。一个典型的神经网络库包含以下几个核心组件：

-   **`nn.Module`**: 所有神经网络层和模型的基类。
-   **`nn.Parameter`**: 特殊的 Tensor，表示模型中可训练的参数。
-   **Loss Functions (损失函数)**: 衡量模型预测与真实标签差距的模块。
-   **Optimizers (优化器)**: 实现梯度下降等更新算法的模块。
-   **Initializers (初始化器)**: 用于初始化模型参数的函数。
-   **Data Loading (数据加载)**: 高效加载和预处理数据的工具。

今天我们重点实现 `Module`, `Parameter`, `Loss` 和 `Optimizer`。

### 3.1 `nn.Module` 和 `nn.Parameter`

`Module` 是我们构建所有神经网络模型的基础。它的核心功能有两个：

1.  **能够像函数一样被调用**，执行前向传播计算 (forward pass)。
2.  **能够自动收集并返回其所有可训练的参数** (parameters)。

为了区分普通的 Tensor 和可训练的参数，我们首先定义一个 `Parameter` 类。它本质上就是一个 `Tensor`，但通过 `isinstance(value, Parameter)` 检查，我们可以识别出它。

```python
# Parameter 类只是一个标记，继承自 ndl.Tensor
class Parameter(ndl.Tensor):
    """一个可训练的参数"""

# _get_params 是一个辅助函数，用于递归地查找参数
def _get_params(value):
    if isinstance(value, Parameter):
        return [value]
    if isinstance(value, dict):
        params = []
        for k, v in value.items():
            params += _get_params(v)
        return params
    if isinstance(value, Module):
        return value.parameters()
    return []

# Module 基类
class Module:
    def parameters(self):
        # self.__dict__ 返回一个包含对象所有属性的字典
        # _get_params 会递归地在这个字典里寻找 Parameter
        return _get_params(self.__dict__)

    def __call__(self, *args, **kwargs):
        # 允许我们像函数一样调用一个模块实例，例如 model(x)
        # 它会自动调用 forward 方法
        return self.forward(*args, **kwargs)
```

> [!EXAMPLE] 例：实现一个简单的 `ScaleAdd` 模块
>
> `ScaleAdd` 模块实现的功能是 $y = x \cdot s + b$，其中 $s$ (scale) 和 $b$ (bias) 是可训练参数。
>
> ```python
> class ScaleAdd(Module):
>     def __init__(self, init_s=1, init_b=0):
>         # 将 s 和 b 定义为 Parameter
>         self.s = Parameter([init_s], dtype="float32")
>         self.b = Parameter([init_b], dtype="float32")
>
>     def forward(self, x):
>         # 定义前向传播
>         return x * self.s + self.b
>
> # 创建实例
> sadd = ScaleAdd(init_s=2, init_b=1)
>
> # 1. 获取参数
> params = sadd.parameters()
> print(params)
> # >> [needle.Tensor([2.]), needle.Tensor([1.])]
>
> # 2. 像函数一样调用
> x = ndl.Tensor([2], dtype="float32")
> y = sadd(x)
> print(y)
> # >> needle.Tensor([5.]) (2 * 2 + 1 = 5)
> ```

### 3.2 模块的组合 (Composing Modules)

`Module` 最强大的地方在于它可以**嵌套**。一个大的、复杂的模型可以由许多小的、简单的模块组合而成。我们的 `parameters()` 方法设计成了递归的，所以它能自动找到所有嵌套子模块中的参数。

> [!EXAMPLE] 例：实现一个多路径的 `MultiPathScaleAdd` 模块
>
> 这个模块包含两个并行的 `ScaleAdd` 路径，最后将它们的结果相加。
>
> ```python
> class MultiPathScaleAdd(Module):
>     def __init__(self):
>         # 包含两个 ScaleAdd 子模块
>         self.path0 = ScaleAdd()
>         self.path1 = ScaleAdd()
>
>     def forward(self, x):
>         # 前向传播：将输入分别送入两个路径，然后相加
>         return self.path0(x) + self.path1(x)
>
> # 创建实例
> mpath = MultiPathScaleAdd()
>
> # 获取参数
> params = mpath.parameters()
> print(params)
> # >> [needle.Tensor([1.]), needle.Tensor([0.]),  # path0 的 s 和 b
> # >>  needle.Tensor([1.]), needle.Tensor([0.])]   # path1 的 s 和 b
>
> # 调用
> x = ndl.Tensor([1], dtype="float32")
> y = mpath(x)
> print(y)
> # >> needle.Tensor([2.])  ((1*1+0) + (1*1+0) = 2)
> ```
>
> 这种模块化的组合能力是你构建残差网络 (ResNet) 等复杂模型的基础。

### 3.3 损失函数 (Loss Function) 和优化器 (Optimizer)

**损失函数**也是一种 `Module`，但通常不包含可训练参数。它接收模型的预测和真实标签，计算出一个标量损失值。

```python
class L2Loss(Module):
    def forward(self, x, y):
        z = x + (-1) * y
        return z * z
```

**优化器**则负责根据计算出的梯度来更新模型的参数。它通常在初始化时接收一个参数列表 (`model.parameters()`)。

优化器主要有两个方法：

-   `reset_grad()`: 在每次计算新梯度之前，清除旧的梯度。
-   `step()`: 执行一次参数更新。

```python
class Optimizer:
    def __init__(self, params):
        self.params = params

    def reset_grad(self):
        for p in self.params:
            p.grad = None

    def step(self):
        raise NotImplemented()

class SGD(Optimizer):
    def __init__(self, params, lr):
        super().__init__(params)
        self.lr = lr

    def step(self):
        for w in self.params:
            # 记住，这里必须使用 .data 进行无图更新！
            w.data = w.data + (-self.lr) * w.grad.data
```

### 3.4 完整的端到端训练流程

现在，我们将所有组件（模型、损失函数、优化器）组合在一起，形成一个完整的训练循环。

> [!CAUTION] 端到端训练示例
>
> ```python
> # 1. 准备数据
> x = ndl.Tensor([2], dtype="float32")
> y = ndl.Tensor([2], dtype="float32")
>
> # 2. 初始化模型、损失函数和优化器
> model = MultiPathScaleAdd()
> l2loss = L2Loss()
> opt = SGD(model.parameters(), lr=0.01)
>
> num_epoch = 10
>
> # 3. 训练循环
> for epoch in range(num_epoch):
>     # a. 清除旧梯度
>     opt.reset_grad()
>
>     # b. 前向传播
>     h = model(x)
>     loss = l2loss(h, y)
>     print(f"Epoch {epoch}, Loss: {loss.numpy()[0]}")
>
>     # c. 反向传播，计算梯度
>     loss.backward()
>
>     # d. 更新参数
>     opt.step()
> ```
>
> **输出:**
>
> ```
> Epoch 0, Loss: 4.0
> Epoch 1, Loss: 2.560000419616699
> Epoch 2, Loss: 1.6384003162384033
> ...
> Epoch 9, Loss: 0.07205747812986374
> ```
>
> 可以看到，损失值随着训练的进行在稳步下降，这证明我们的整个系统工作正常！这个模块化的设计非常灵活，你可以轻松地替换模型、损失函数或优化器，而无需改动训练循环的主体结构。

### 3.5 参数初始化

在神经网络中，权重的初始值对训练的成败至关重要。一个好的初始化可以帮助模型更快地收敛，避免梯度消失或爆炸。

对于一个使用 ReLU 激活函数的线性网络，一个常用的初始化方法是**Kaiming 初始化** (或 He 初始化)。其核心思想是让每一层的输出方差约等于输入方差。

对于一个线性层 $y = Wx$，其输出 $y_i$ 的方差可以表示为：

$\text{Var}[y_i] = n_{in} \cdot E[x^2] \cdot \text{Var}[W_{ij}]$

如果输入 $x$ 是前一层 ReLU 的输出，那么 $E[x^2] \approx \frac{1}{2}\text{Var}[y^{(l-1)}]$。为了保持方差稳定 ($\text{Var}[y^{(l)}] = \text{Var}[y^{(l-1)}]$)，我们可以推导出权重的方差应该是：

$\text{Var}[W] = \frac{2}{n_{in}}$

如果权重从均值为 0 的高斯分布 $\mathcal{N}(0, \sigma^2)$ 中采样，那么标准差 $\sigma$ 就应该是 $\sqrt{\frac{2}{n_{in}}}$。

---

## Part 4: 进阶内容 - 元组值 (Tuple Value)

在 `needle` 中，我们还可以让操作符返回多个输出。为了支持这个功能，框架引入了一个新的值类型：`TensorTuple`。

这使得我们可以实现**融合算子 (Fused Operator)**，即一次计算返回多个结果，这在性能优化上很有用。

> [!EXAMPLE] `fused_add_scalars`
>
> 这个算子接收一个张量 `x` 和两个标量 `c0`, `c1`，然后同时返回 `x+c0` 和 `x+c1`。
>
> ```python
> x = ndl.Tensor([1], dtype="float32")
>
> # z 是一个 TensorTuple
> z = ndl.ops.fused_add_scalars(x, 1, 2)
>
> # 可以通过索引获取其中的 Tensor
> v0 = z[0]  # v0 是 x + 1
> v1 = z[1]  # v1 是 x + 2
>
> # 自动微分机制同样适用于这种情况
> loss = v0 + v1 * 2
> loss.backward()
> print(x.grad)
> # >> needle.Tensor([3.]) (dL/dv0 * dv0/dx + dL/dv1 * dv1/dx = 1*1 + 2*1 = 3)
> ```
>
> 令人惊叹的是，我们现有的自动微分系统无需修改就能完美支持这种多输出算子，这展示了框架设计的优雅之处。

---

> [!SUMMARY] 总结
>
> 恭喜你完成了本次讲座的学习！我们一起：
> 1.  学习了权重更新的正确姿势，理解了 `detach` 的重要性。
> 2.  探讨了 `Softmax` 中的数值稳定性问题及其解决方案。
> 3.  从零开始，设计并实现了一个模块化的神经网络库，包括 `Module`、`Parameter`、`L2Loss` 和 `SGD`。
> 4.  将所有组件串联起来，完成了一个端到端的训练流程。
> 5.  了解了参数初始化的基本原理和进阶的元组值概念。
>
> 这些知识和技能是你未来构建、理解和调试深度学习系统的坚实基础。现在，你可以充满信心地去完成相关的作业了！
