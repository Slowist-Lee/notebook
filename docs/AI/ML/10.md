# Diffusion Model


扩散模型（Diffusion Models）的灵感来自于非平衡热力学。它们定义了一个扩散步骤的马尔可夫链（Markov chain），慢慢地向数据中添加随机噪声，然后学习逆向扩散过程，从噪声中构建所需的数据样本。与 VAE 或流模型不同，扩散模型是通过固定过程学习的，并且潜在变量具有高维度（与原始数据相同）。

![](Pasted%20image%2020260209180430.png)

## 什么是扩散模型？

近年来提出了几种具有类似底层思路的基于扩散的生成模型，包括 **扩散概率模型 (diffusion probabilistic models)** (Sohl-Dickstein et al., 2015), **噪声条件分数网络 (noise-conditioned score network)** (**NCSN**; Yang & Ermon, 2019), 以及 **去噪扩散概率模型 (denoising diffusion probabilistic models)** (**DDPM**; Ho et al. 2020)。

### 前向扩散过程 (Forward diffusion process)

给定一个从真实数据分布中采样的数据点 $\mathbf{x}_0 \sim q(\mathbf{x})$，我们定义一个 *前向扩散过程*，在这个过程中，我们分 $T$ 步向样本中添加少量的由方差调度表（variance schedule）$\{\beta_t \in (0, 1)\}_{t=1}^T$ 控制的高斯噪声，产生一系列带噪声的样本 $\mathbf{x}_1, \dots, \mathbf{x}_T$。

$$ q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}) \quad q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) $$

随着步数 $t$ 变大，数据样本 $\mathbf{x}_0$ 逐渐失去其可区分的特征。最终当 $T \to \infty$ 时，$\mathbf{x}_T$ 等价于各向同性的高斯分布（isotropic Gaussian distribution）。

![](Pasted%20image%2020260209180444.png)
*(图2: 正向（生成）样本的马尔可夫链，通过缓慢添加（去除）噪声。图片来源: Ho et al. 2020)*

上述过程的一个**优良性质**是，我们可以在任意时间步 $t$ 以闭式解（closed form）采样 $\mathbf{x}_t$，这得益于重参数化技巧。令 $\alpha_t = 1 - \beta_t$ 且 $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$：

$$
\begin{aligned}
\mathbf{x}_t &= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} & ; \text{where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}\mathbf{x}_{t-2} + \sqrt{1 - \alpha_{t-1}}\boldsymbol{\epsilon}_{t-2}) + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} & ; \text{where } \bar{\alpha}_{t-2} \text{ merges two Gaussians (*).} \\
&= \dots \\
&= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} \\
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
$$

> 回忆一下，当我们合并两个高斯分布 $\mathcal{N}(\mathbf{0}, \sigma_1^2\mathbf{I})$ 和 $\mathcal{N}(\mathbf{0}, \sigma_2^2\mathbf{I})$ 时，新的分布是 $\mathcal{N}(\mathbf{0}, (\sigma_1^2 + \sigma_2^2)\mathbf{I})$。这里合并后的标准差为 $\sqrt{(1 - \alpha_t) + \alpha_t (1-\alpha_{t-1})} = \sqrt{1 - \alpha_t\alpha_{t-1}}$。

通常，随着样本变得更嘈杂，我们可以采用更大的更新步长，因此 $\beta_1 < \beta_2 < \dots < \beta_T$，这意味着 $\bar{\alpha}_1 > \dots > \bar{\alpha}_T$。

#### 与随机梯度朗之万动力学 (Stochastic Gradient Langevin Dynamics) 的联系

朗之万动力学（Langevin dynamics）是一个来自物理学的概念，用于对分子系统进行统计建模。结合随机梯度下降，*随机梯度朗之万动力学* (Welling & Teh 2011) 可以仅使用梯度 $\nabla_\mathbf{x} \log p(\mathbf{x})$ 从概率密度 $p(\mathbf{x})$ 中生成样本，更新过程如下：

$$ \mathbf{x}_{t} = \mathbf{x}_{t-1} + \frac{\delta}{2} \nabla_\mathbf{x} \log p(\mathbf{x}_{t-1}) + \sqrt{\delta} \boldsymbol{\epsilon}_t, \quad \text{where } \boldsymbol{\epsilon}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) $$

其中 $\delta$ 是步长。当 $T \to \infty, \epsilon \to 0$ 时，$\mathbf{x}_T$ 等于真实的概率密度 $p(\mathbf{x})$。

与标准 SGD 相比，随机梯度朗之万动力学将高斯噪声注入到参数更新中，以避免陷入局部极小值。

### 逆向扩散过程 (Reverse diffusion process)

如果我们能够逆转上述过程并从 $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$ 中采样，我们将能够从高斯噪声输入 $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ 中重建真实样本。注意，如果 $\beta_t$ 足够小，$q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$ 也将是高斯分布。不幸的是，我们无法轻易估计 $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$，因为它需要使用整个数据集，因此我们需要学习一个模型 $p_\theta$ 来近似这些条件概率，以便运行 *逆向扩散过程*。

$$ p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod^T_{t=1} p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) \quad p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)) $$

![](Pasted%20image%2020260209180503.png)

值得注意的是，当以 $\mathbf{x}_0$ 为条件时，逆向条件概率是易于计算的（tractable）：

$$ q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \color{blue}{\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0)}, \color{red}{\tilde{\beta}_t} \mathbf{I}) $$

使用贝叶斯规则，我们有：

$$
\begin{aligned}
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) &= q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0) \frac{ q(\mathbf{x}_{t-1} \vert \mathbf{x}_0) }{ q(\mathbf{x}_t \vert \mathbf{x}_0) } \\
&\propto \exp \Big(-\frac{1}{2} \big(\frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{\beta_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0)^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) \\
&= \exp \Big(-\frac{1}{2} \big(\frac{\mathbf{x}_t^2 - 2\sqrt{\alpha_t} \mathbf{x}_t \color{blue}{\mathbf{x}_{t-1}} \color{black}{+ \alpha_t} \color{blue}{\mathbf{x}_{t-1}^2}}{\beta_t} + \frac{\color{blue}{\mathbf{x}_{t-1}^2} \color{black}{- 2\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0} \color{blue}{\mathbf{x}_{t-1}} \color{black}{+ \bar{\alpha}_{t-1} \mathbf{x}_0^2} }{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) \\
&= \exp\Big( -\frac{1}{2} \big( (\color{red}{\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}}\color{black}{)} \color{blue}{\mathbf{x}_{t-1}^2} - (\color{red}{\frac{2\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}}\mathbf{x}_0}\color{black}{)} \color{blue}{\mathbf{x}_{t-1}} \color{black}{ + C(\mathbf{x}_t, \mathbf{x}_0) \big) \Big)}
\end{aligned}
$$

其中 $C(\mathbf{x}_t, \mathbf{x}_0)$ 是某个不涉及 $\mathbf{x}_{t-1}$ 的函数，细节省略。遵循标准高斯密度函数，均值和方差可以参数化如下（回忆 $\alpha_t = 1 - \beta_t$ 和 $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$）：

$$
\begin{aligned}
\tilde{\beta}_t &= 1/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}) = 1/(\frac{\alpha_t - \bar{\alpha}_t + \beta_t}{\beta_t(1 - \bar{\alpha}_{t-1})}) = \color{green}{\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t} \\
\tilde{\boldsymbol{\mu}}_t (\mathbf{x}_t, \mathbf{x}_0) &= (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}) \\
&= (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0) \color{green}{\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t} \\
&= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0
\end{aligned}
$$

由于拥有这个[优良性质](#nice-property)，我们可以表示 $\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t)$ 并将其代入上述方程得到：

$$
\begin{aligned}
\tilde{\boldsymbol{\mu}}_t &= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t) \\
&= \color{cyan}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)}
\end{aligned}
$$

如图2所示，这种设置非常类似于 VAE，因此我们可以使用变分下界（Variational Lower Bound）来优化负对数似然。

$$
\begin{aligned}
- \log p_\theta(\mathbf{x}_0) &\le - \log p_\theta(\mathbf{x}_0) + D_\text{KL}(q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{1:T} \vert \mathbf{x}_0)) \\
&= - \log p_\theta(\mathbf{x}_0) + \mathbb{E}_{\mathbf{x}_{1:T} \sim q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \Big[ \log \frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T}) / p_\theta(\mathbf{x}_0)} \Big] \\
&= - \log p_\theta(\mathbf{x}_0) + \mathbb{E}_q \Big[ \log \frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} + \log p_\theta(\mathbf{x}_0) \Big] \\
&= \mathbb{E}_q \Big[ \log \frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
\text{Let } L_{\text{VLB}} &= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log \frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \ge - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0)
\end{aligned}
$$

使用詹森不等式（Jensen's inequality）也能直接得出相同结果。假设我们要最小化交叉熵作为学习目标：

$$
\begin{aligned}
L_\text{CE} &= - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0) \\
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \int p_\theta(\mathbf{x}_{0:T}) d\mathbf{x}_{1:T} \Big) \\
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \int q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} d\mathbf{x}_{1:T} \Big) \\
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \mathbb{E}_{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \Big) \\
&\le - \mathbb{E}_{q(\mathbf{x}_{0:T})} \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \\
&= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log \frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] = L_{\text{VLB}}
\end{aligned}
$$

为了将方程中的每一项转换为可解析计算的形式，可以将目标重写为几个 KL 散度和熵项的组合（详细步骤见 Sohl-Dickstein et al. 2015 的附录B）：

$$
\begin{aligned}
L_{\text{VLB}} &= \mathbb{E}_q \Big[ \log \frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
&= \mathbb{E}_q \Big[ \log \frac{\prod_{t=1}^T q(\mathbf{x}_t \vert \mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)} \Big] \\
&= \mathbb{E}_q \Big[ - \log p_\theta(\mathbf{x}_T) + \sum_{t=1}^T \log \frac{q(\mathbf{x}_t \vert \mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)} \Big] \\
&= \mathbb{E}_q \Big[ - \log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t \vert \mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)} + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ - \log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \Big( \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)} \cdot \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} \Big) + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ - \log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ - \log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)} + \log \frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{q(\mathbf{x}_1 \vert \mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ \log \frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \Big] \\
&= \mathbb{E}_q \Big[ \underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} \Big]
\end{aligned}
$$

我们将变分下界中的每个组件分别标记为：

$$
\begin{aligned}
L_{\text{VLB}} &= L_T + L_{T-1} + \dots + L_0 \\
\text{where } L_T &= D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T)) \\
L_t &= D_\text{KL}(q(\mathbf{x}_t \vert \mathbf{x}_{t+1}, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_t \vert \mathbf{x}_{t+1})) \text{ for } 1 \le t \le T-1 \\
L_0 &= - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)
\end{aligned}
$$

$L_{\text{VLB}}$ 中的每一项（除了 $L_0$）都是比较两个高斯分布，因此它们可以以**闭式解**计算。$L_T$ 是常数，在训练过程中可以忽略，因为 $q$ 没有可学习的参数，$p_\theta(\mathbf{x}_T)$ 是高斯噪声。[Ho et al. 2020](https://arxiv.org/abs/2006.11239) 使用单独的离散解码器来建模 $L_0$，该解码器源自 $\mathcal{N}(\mathbf{x}_0; \boldsymbol{\mu}_\theta(\mathbf{x}_1, 1), \boldsymbol{\Sigma}_\theta(\mathbf{x}_1, 1))$。

### $L_t$ 的参数化以用于训练损失

回想一下，我们需要学习一个神经网络来近似逆向扩散过程中的条件概率分布 $p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))$。我们希望训练 $\boldsymbol{\mu}_\theta$ 来预测 $\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}} \big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \big)$。因为 $\mathbf{x}_t$ 在训练时作为输入是可用的，我们可以重新参数化高斯噪声项，使其预测 $\boldsymbol{\epsilon}_t$（来自时间步 $t$ 的输入 $\mathbf{x}_t$）：

$$
\begin{aligned}
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) &= \color{cyan}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big)} \\
\text{Thus } \mathbf{x}_{t-1} &= \mathcal{N}(\mathbf{x}_{t-1}; \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))
\end{aligned}
$$

损失项 $L_t$ 被参数化为最小化 $\tilde{\boldsymbol{\mu}}$ 与 $\boldsymbol{\mu}_\theta$ 之间的差异：

$$
\begin{aligned}
L_t &= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[ \frac{1}{2 \| \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) \|^2_2} \| \color{blue}{\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0)} - \color{green}{\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)} \|^2 \Big] \\
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[ \frac{1}{2 \| \boldsymbol{\Sigma}_\theta \|^2_2} \| \color{blue}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)} - \color{green}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big)} \|^2 \Big] \\
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[ \frac{ (1 - \alpha_t)^2 }{ 2 \alpha_t (1 - \bar{\alpha}_t) \| \boldsymbol{\Sigma}_\theta \|^2_2} \| \boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \|^2 \Big] \\
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[ \frac{ (1 - \alpha_t)^2 }{ 2 \alpha_t (1 - \bar{\alpha}_t) \| \boldsymbol{\Sigma}_\theta \|^2_2} \| \boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t) \|^2 \Big]
\end{aligned}
$$

#### 简化 (Simplification)

根据经验，[Ho et al. (2020)](https://arxiv.org/abs/2006.11239) 发现训练扩散模型时如果忽略加权项效果更好：

$$
\begin{aligned}
L^{\text{simple}}_t &= \mathbb{E}_{t \sim [1, T], \mathbf{x}_0, \boldsymbol{\epsilon}_t} \Big[ \| \boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \|^2 \Big] \\
&= \mathbb{E}_{t \sim [1, T], \mathbf{x}_0, \boldsymbol{\epsilon}_t} \Big[ \| \boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t) \|^2 \Big]
\end{aligned}
$$

最终的简单目标函数是：

$$ L_{\text{simple}} = L^{\text{simple}}_t + C $$

其中 $C$ 是一个不依赖于 $\theta$ 的常数。

![](Pasted%20image%2020260209180619.png)
*(图4: DDPM 中的训练和采样算法 (图片来源: Ho et al. 2020))*

### 与噪声条件分数网络 (NCSN) 的联系

[Song & Ermon (2019)](https://arxiv.org/abs/1907.05600) 提出了一种基于分数的生成模型方法，通过[朗之万动力学](#connection-with-stochastic-gradient-langevin-dynamics)生成样本，使用数据分布的梯度（即分数 score）进行估计。每个样本 $\mathbf{x}$ 的概率密度函数的梯度为 $\nabla_\mathbf{x} \log q(\mathbf{x})$。训练一个分数网络 $s_\theta: \mathbb{R}^D \to \mathbb{R}^D$ 来估计它，即 $s_\theta(\mathbf{x}) \approx \nabla_\mathbf{x} \log q(\mathbf{x})$。

为了在大维度的深度学习环境中使其可扩展，他们建议使用 *去噪分数匹配 (denoising score matching)* (Vincent, 2011) 或 *切片分数匹配 (sliced score matching)* (使用随机投影; Song et al., 2019)。去噪分数匹配向数据中添加预先指定的少量噪声 $q(\tilde{\mathbf{x}} \vert \mathbf{x})$ 并用分数匹配估计 $q(\tilde{\mathbf{x}})$。

回想一下，朗之万动力学可以仅使用分数 $\nabla_\mathbf{x} \log q(\mathbf{x})$ 在迭代过程中从概率密度分布中采样数据点。

然而，根据流形假设，大多数数据往往集中在低维流形上，即使观察到的数据看起来可能具有任意高的维度。这对分数估计带来了负面影响，因为数据点无法覆盖整个空间。在数据密度低的区域，分数估计不太可靠。在向数据添加少量高斯噪声使其扰动后的数据分布覆盖整个空间 $\mathbb{R}^D$ 后，分数估计器的训练变得更加稳定。[Song & Ermon (2019)](https://arxiv.org/abs/1907.05600) 通过用 *不同级别* 的噪声扰动数据并训练噪声条件分数网络来 *联合* 估计所有噪声水平下扰动数据的分数，从而改进了这一点。

增加噪声水平的调度表类似于扩散模型的前向扩散过程。如果我们使用扩散过程注解，分数估计即为 $s_\theta(\mathbf{x}_t, t) \approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t)$。给定高斯分布 $\mathbf{x} \sim \mathcal{N}(\mu, \sigma^2)$，我们可以将其密度函数的导数写为 $\nabla_\mathbf{x} \log p(\mathbf{x}) = \nabla_\mathbf{x} \big( -\frac{(\mathbf{x} - \mu)^2}{2\sigma^2} \big) = - \frac{\mathbf{x} - \mu}{\sigma^2} = - \frac{\epsilon}{\sigma}$，其中 $\epsilon \sim \mathcal{N}(0, 1)$。回想 $q(\mathbf{x}_t \vert \mathbf{x}_0) \sim \mathcal{N}(\sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})$，因此，

$$ s_\theta(\mathbf{x}_t, t) \approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) = \mathbb{E}_{q(\mathbf{x}_0)} [ \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \vert \mathbf{x}_0) ] = \mathbb{E}_{q(\mathbf{x}_0)} [ - \frac{\boldsymbol{\epsilon}_t}{\sqrt{1 - \bar{\alpha}_t}} ] = - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}} $$

### $\beta_t$ 的参数化

前向方差被设定为一系列线性增加的常数，在 [Ho et al. (2020)](https://arxiv.org/abs/2006.11239) 中，从 $\beta_1 = 10^{-4}$ 到 $\beta_T = 0.02$。与归一化后的图像像素值范围 $[-1, 1]$ 相比，它们相对较小。扩散模型在实验中显示出高质量的样本，但与其它的生成模型相比，并未能达到具有竞争力的模型对数似然值。

[Nichol & Dhariwal (2021)](https://arxiv.org/abs/2102.09672) 提出了几种改进技术来帮助扩散模型获得更低的 NLL。其中一项改进是使用余弦方差调度表（cosine-based variance schedule）。调度函数的选择可以是任意的，只要它在训练过程中提供近似线性的下降，并且在 $t=0$ 和 $t=T$ 附近变化平缓。

$$ \beta_t = \text{clip}(1 - \frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}, 0.999), \quad \bar{\alpha}_t = \frac{f(t)}{f(0)}, \quad \text{where } f(t) = \cos^2 \big( \frac{t/T + s}{1 + s} \cdot \frac{\pi}{2} \big) $$

其中小偏移量 $s$ 是为了防止 $\beta_t$ 在接近 $t=0$ 时变得太小。

![|500](Pasted%20image%2020260209180628.png)
*(图5: 训练过程中线性和基于余弦的 $\beta_t$ 调度比较。图片来源: Nichol & Dhariwal, 2021)*

### 逆向过程方差 $\Sigma_\theta$ 的参数化

[Ho et al. (2020)](https://arxiv.org/abs/2006.11239) 选择将 $\beta_t$ 固定为常数而不是让它们可学习，并设置 $\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) = \sigma_t^2 \mathbf{I}$，其中 $\sigma_t$ 不可学习，设置为 $\beta_t$ 或 $\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t$。因为他们发现学习对角方差 $\boldsymbol{\Sigma}_\theta$ 会导致训练不稳定和样本质量变差。

[Nichol & Dhariwal (2021)](https://arxiv.org/abs/2102.09672) 提出将 $\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)$ 学习为 $\beta_t$ 和 $\tilde{\beta}_t$ 之间的插值，模型预测混合向量 $\mathbf{v}$：

$$ \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) = \exp(\mathbf{v} \log \beta_t + (1 - \mathbf{v}) \log \tilde{\beta}_t) $$

然而，简单的目标函数 $L_{simple}$ 不依赖于 $\Sigma_\theta$。为了添加依赖关系，他们构建了一个混合目标函数 $L_{hybrid} = L_{simple} + \lambda L_{VLB}$，其中 $\lambda = 0.001$ 并且在 $L_{VLB}$ 项上停止梯度回传，使得 $L_{VLB}$ 仅指导 $\Sigma_\theta$ 的学习。根据经验，他们观察到 $L_{VLB}$ 由于噪声梯度的原因很难优化，所以他们提出使用 $L_{VLB}$ 的时间平均平滑版本（importance sampling）。

![|475](Pasted%20image%2020260209180637.png)
*(图6: 改进的 DDPM 与其他基于似然的生成模型的负对数似然比较。NLL 单位为 bits/dim。图片来源: Nichol & Dhariwal, 2021)*

## 条件生成 (Conditioned Generation)

在针对 ImageNet 数据集等带有条件信息的图像训练生成模型时，通常会生成以类别标签或一段描述性文本为条件的样本。

### 分类器引导扩散 (Classifier Guided Guidance)

为了显式地将类别信息融入扩散过程，[Dhariwal & Nichol (2021)](https://arxiv.org/abs/2105.05233) 在噪声图像 $\mathbf{x}_t$ 上训练了一个分类器 $f_\phi(y \vert \mathbf{x}_t)$，并使用梯度 $\nabla_\mathbf{x} \log f_\phi(y \vert \mathbf{x}_t)$ 来引导扩散采样过程朝向目标类别 $y$（例如目标类别标签）。回想 $\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) = - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$，我们可以将联合分布 $p(\mathbf{x}, y)$ 的分数函数写为：

$$
\begin{aligned}
\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t, y) &= \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) + \nabla_{\mathbf{x}_t} \log q(y \vert \mathbf{x}_t) \\
&\approx - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) + \nabla_{\mathbf{x}_t} \log f_\phi(y \vert \mathbf{x}_t) \\
&= - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} (\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) - \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{x}_t} \log f_\phi(y \vert \mathbf{x}_t))
\end{aligned}
$$

因此，新的分类器引导的预测器 $\bar{\boldsymbol{\epsilon}}_\theta$ 将采用以下形式：

$$ \bar{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t) = \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) - \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{x}_t} \log f_\phi(y \vert \mathbf{x}_t) $$

为了控制分类器引导的强度，我们可以添加一个权重 $w$ 到 delta 部分：

$$ \bar{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t) = \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) - \sqrt{1 - \bar{\alpha}_t} \cdot w \nabla_{\mathbf{x}_t} \log f_\phi(y \vert \mathbf{x}_t) $$

由此产生的 **Ablated Diffusion Model (ADM)** 以及带有额外分类器引导的 (**ADM-G**) 能够获得优于 SOTA 生成模型（如 BigGAN）的结果。

![](Pasted%20image%2020260209180647.png)
*(图7: 使用 DDPM 和 DDIM 进行条件生成的算法。图片来源: Dhariwal & Nichol, 2021)*

此外，对 U-Net 架构进行了一些修改，[Dhariwal & Nichol (2021)](https://arxiv.org/abs/2105.05233) 表现出比带有扩散模型的 GAN 更好的性能。架构修改包括更大的模型深度/宽度、更多的注意力头、多分辨率注意力、BigGAN 残差块（用于上/下采样）、残差连接按 $1/\sqrt{2}$ 缩放以及自适应组归一化（AdaGN）。

### 无分类器引导 (Classifier-Free Guidance)

如果不需要独立的分类器 $f_\phi$，也可以通过将分数并入条件和无条件扩散模型中来运行条件扩散步骤 ([Ho & Salimans, 2021](https://openreview.net/forum?id=qw8AKxfYbI))。设无条件扩散模型 $p_\theta(\mathbf{x})$ 参数化为分数估计器 $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$，条件模型 $p_\theta(\mathbf{x} \vert y)$ 参数化为 $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y)$。这两个模型可以通过单个神经网络学习。精确地说，条件扩散模型 $p_\theta(\mathbf{x} \vert y)$ 是在配对数据 $(\mathbf{x}, y)$ 上训练的，其中条件信息 $y$ 会以概率 $p_{uncond}$ 随机丢弃，这样模型也就知道如何无条件地生成图像，即 $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) = \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y = \emptyset)$。

隐式分类器的梯度可以用条件和无条件分数估计器表示。一旦插入分类器引导修改后的分数中，分数不再依赖于单独的分类器。

$$
\begin{aligned}
\nabla_{\mathbf{x}_t} \log p(y \vert \mathbf{x}_t) &= \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t \vert y) - \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t) \\
&= - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} (\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y) - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)) \\
\bar{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t, y) &= \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y) - \sqrt{1 - \bar{\alpha}_t} \cdot w \nabla_{\mathbf{x}_t} \log p(y \vert \mathbf{x}_t) \\
&= \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y) + w(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y) - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)) \\
&= (w + 1) \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y) - w \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)
\end{aligned}
$$

他们的实验表明，无分类器引导可以在 FID（区分合成图像和生成图像）和 IS（质量和多样性）之间取得良好的平衡。

引导扩散模型 GLIDE ([Nichol, Dhariwal & Ramesh, et al. 2022](https://arxiv.org/abs/2112.10741)) 探索了引导策略、CLIP 引导和无分类器引导，发现后者更受青睐。他们假设这是因为 CLIP 引导利用了对抗样本向 CLIP 模型靠拢，而不是优化以获得更好的匹配图像生成。

## 加速扩散模型 (Speed up Diffusion Models)

通过遵循逆向扩散过程的马尔可夫链从 DDPM 生成样本非常慢，因为 $T$ 可能高达一千或几千步。来自 [Song et al. (2020)](https://arxiv.org/abs/2010.02502) 的一个数据点："例如，在 Nvidia 2080 Ti GPU 上采样 50k 张 32x32 的图像大约需要 20 小时，而在 GAN 上只需要不到一分钟。"

### 更少的采样步骤和蒸馏 (Fewer Sampling Steps & Distillation)

一种简单的方法是运行跨步采样计划 ([Nichol & Dhariwal, 2021](https://arxiv.org/abs/2102.09672))，每 $[T/S]$ 步进行一次采样更新，将过程从 $T$ 步减少到 $S$ 步。生成的新采样计划是 $\{\tau_1, \dots, \tau_S\}$，其中 $\tau_1 < \tau_2 < \dots < \tau_S \in [1, T]$ 且 $S < T$。

对于另一种方法，让我们根据所需的标准差 $\sigma_t$ 重写 $q_\sigma(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)$，根据[优良性质](#nice-property)：

$$
\begin{aligned}
\mathbf{x}_{t-1} &= \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1}}\boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2}\boldsymbol{\epsilon}_t + \sigma_t\boldsymbol{\epsilon} \\
&= \sqrt{\bar{\alpha}_{t-1}} \Big( \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_\theta^{(t)}(\mathbf{x}_t)}{\sqrt{\bar{\alpha}_t}} \Big) + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2}\boldsymbol{\epsilon}_\theta^{(t)}(\mathbf{x}_t) + \sigma_t\boldsymbol{\epsilon} \\
q_\sigma(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}} \Big( \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_\theta^{(t)}(\mathbf{x}_t)}{\sqrt{\bar{\alpha}_t}} \Big) + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2}\boldsymbol{\epsilon}_\theta^{(t)}(\mathbf{x}_t), \sigma_t^2 \mathbf{I})
\end{aligned}
$$

其中模型 $\epsilon_\theta^{(t)}(\cdot)$ 预测来自 $\mathbf{x}_t$ 的 $\epsilon_t$。

回想一下在 $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t \mathbf{I})$ 中，我们有：

$$ \tilde{\beta}_t = \sigma_t^2 = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t $$

令 $\sigma_t^2 = \eta \cdot \tilde{\beta}_t$，这样我们可以调整 $\eta \in \mathbb{R}^+$ 作为超参数来控制采样随机性。$\eta = 0$ 的特殊情况使得采样过程具有 *确定性*。这种模型被称为 **去噪扩散隐式模型 (Denoising Diffusion Implicit Model)** (**DDIM**; [Song et al., 2020](https://arxiv.org/abs/2010.02502))。DDIM 具有相同的边缘噪声分布，但确定性地将噪声映射回原始数据样本。

在生成过程中，我们不需要遵循 $t = 1, \dots, T$ 的整个链，而是可以取子集。设 $s < t$ 为这个加速轨迹中的两个步骤。DDIM 更新步骤是：

$$ q_{\sigma, s < t}(\mathbf{x}_s \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_s; \sqrt{\bar{\alpha}_s} \Big( \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_\theta^{(t)}(\mathbf{x}_t)}{\sqrt{\bar{\alpha}_t}} \Big) + \sqrt{1 - \bar{\alpha}_s - \sigma_t^2}\boldsymbol{\epsilon}_\theta^{(t)}(\mathbf{x}_t), \sigma_t^2 \mathbf{I}) $$

虽然所有模型都在实验中使用了 $T=1000$ 个扩散步骤进行训练，但他们观察到，当 $S$ 很小时，DDIM ($\eta=0$) 可以产生最好的质量样本，而 DDPM ($\eta=1$) 表现差得多。但是当我们可以负担运行完整的反向马尔可夫扩散步骤 ($S=T=1000$) 时，DDPM 确实表现得更好。有了 DDIM，就可以将扩散模型训练为任意数量的前向步骤，但仅从生成过程中的一部分步骤中采样。

![](Pasted%20image%2020260209180658.png)
*(图8: 不同设置下扩散模型在 CIFAR10 和 CelebA 数据集上的 FID 分数，包括 DDIM ($\eta=0$) 和 DDPM ($\eta=1$)。图片来源: Song et al., 2020)*

与 DDPM 相比，DDIM 能够：
1.  使用更少的步骤生成更高质量的样本。
2.  具有“一致性”属性，因为生成过程是确定性的，意味着以相同潜在变量为条件的多个样本应具有类似的高级特征。
3.  由于一致性，DDIM 可以在潜在变量中进行语义上有意义的插值。

![](Pasted%20image%2020260209180704.png)
*(图9: 渐进式蒸馏采样步骤，每一步减半。图片来源: Salimans & Ho, 2022)*

**渐进式蒸馏 (Progressive Distillation)** ([Salimans & Ho, 2022](https://arxiv.org/abs/2202.00512)) 是一种将训练好的确定性采样器蒸馏成新的采样步骤减半的模型的方法。学生模型被初始化为教师模型，并朝着目标进行去噪，其中一个学生 DDIM 步骤匹配 2 个教师步骤，而不是像去噪目标是原始样本 $\mathbf{x}_0$ 那样。在每一次渐进式蒸馏迭代中，我们可以将采样步骤减半。

![](Pasted%20image%2020260209180710.png)
*(图10: 算法1 (扩散模型训练) 和 算法2 (渐进式蒸馏) 的并排比较，其中渐进式蒸馏中的相对变化用绿色高亮显示。图片来源: Salimans & Ho, 2022)*

### 一致性模型 (Consistency Models)

**一致性模型** ([Song et al. 2023](https://arxiv.org/abs/2301.01469)) 学习将任何中间噪声数据点 $\mathbf{x}_t, t > 0$ 在扩散采样轨迹上映射回其原点 $\mathbf{x}_0$。它直接被命名为一致性模型，是因为其 *自洽性 (self-consistency)* 属性，即同一轨迹上的任何数据点都映射到同一个原点。

![](Pasted%20image%2020260209180715.png)
*(图11: 一致性模型学习将轨迹上的任何数据点映射到其原点。图片来源: Song et al., 2023)*

给定轨迹 $\{\mathbf{x}_t \vert t \in [\epsilon, T]\}$，*一致性函数* $f$ 定义为 $f: (\mathbf{x}_t, t) \mapsto \mathbf{x}_\epsilon$。方程 $f(\mathbf{x}_t, t) = f(\mathbf{x}_{t'}, t') = \mathbf{x}_\epsilon$ 对所有 $t, t' \in [\epsilon, T]$ 成立。当 $t = \epsilon$ 时，即为恒等函数。该模型可以参数化如下，其中 $c_{skip}(t)$ 和 $c_{out}(t)$ 函数被设计为 $c_{skip}(\epsilon) = 1, c_{out}(\epsilon) = 0$：

$$ f_\theta(\mathbf{x}, t) = c_{skip}(t)\mathbf{x} + c_{out}(t) F_\theta(\mathbf{x}, t) $$

一致性模型可以生成单步样本，同时保持计算灵活性，通过多步采样过程以获得更好的质量。

论文介绍了两种训练一致性模型的方法：

1.  **一致性蒸馏 (Consistency Distillation, CD):** 将扩散模型蒸馏为一致性模型，方法是最小化从同一轨迹生成的输出对之间的差异。这就能够进行更廉价的采样评估。一致性蒸馏损失为：

    $$ \mathcal{L}_{CD}^N(\theta, \theta^-; \phi) = \mathbb{E}[\lambda(t_n) d(f_\theta(\mathbf{x}_{t_{n+1}}, t_{n+1}), f_{\theta^-}(\hat{\mathbf{x}}_{t_n}^\phi, t_n))] $$
    $$ \hat{\mathbf{x}}_{t_n}^\phi = \mathbf{x}_{t_{n+1}} - (t_n - t_{n+1}) \Phi(\mathbf{x}_{t_{n+1}}, t_{n+1}; \phi) $$
    
    其中
    *   $\Phi(\cdot; \phi)$ 是一步 ODE 求解器的更新函数；
    *   $n \sim \mathcal{U}[1, N-1]$，在 $1, \dots, N-1$ 上均匀分布；
    *   网络参数 $\theta^-$ 是 $\theta$ 的 EMA 版本，这极大地稳定了训练（就像在 DQN 或动量对比学习中一样）；
    *   $d(\cdot, \cdot)$ 是正距离度量函数，满足 $\forall \mathbf{x}, \mathbf{y}: d(\mathbf{x}, \mathbf{y}) \ge 0$ 且 $d(\mathbf{x}, \mathbf{y}) = 0$ 当且仅当 $\mathbf{x} = \mathbf{y}$，例如 $\ell_2, \ell_1$ 或 LPIPS (学习到的感知图像块相似度) 距离；
    *   $\lambda(\cdot) \in \mathbb{R}^+$ 是正加权函数，且论文设定 $\lambda(t_n) = 1$。

2.  **一致性训练 (Consistency Training, CT):** 另一个选项是独立训练一致性模型。注意在 CD 中，使用预训练的分数模型 $s_\phi(\mathbf{x}, t)$ 来近似真实分数 $\nabla \log p_t(\mathbf{x})$，但在 CT 中我们需要一种方法来估计这个分数函数，这证明了 $\nabla \log p_t(\mathbf{x})$ 的无偏估计量以 $-\frac{\mathbf{x} - \mathbf{x}_0}{\sigma_t^2}$ 的形式存在。CT 损失定义如下：

    $$ \mathcal{L}_{CT}^N(\theta, \theta^-; \phi) = \mathbb{E}[\lambda(t_n) d(f_\theta(\mathbf{x} + t_{n+1}\mathbf{z}, t_{n+1}), f_{\theta^-}(\mathbf{x} + t_n\mathbf{z}, t_n))] \text{ where } \mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) $$

根据论文中的实验，他们发现：
*   Heun ODE 求解器比 Euler 一阶求解器效果更好，因为高阶 ODE 求解器在相同 $N$ 下估计误差更小。
*   在距离度量函数 $d(\cdot, \cdot)$ 的不同选项中，LPIPS 度量优于 $\ell_1$ 和 $\ell_2$ 距离。
*   更小的 $N$ 导致更快的收敛但样本更差，而更大的 $N$ 导致更慢的收敛但收敛后样本更好。

![](Pasted%20image%2020260209180725.png)
*(图12: 不同配置下一致性模型的性能比较。CD 的最佳配置是 LPIPS 距离度量，Heun ODE 求解器，以及 $N=18$。(图片来源: Song et al., 2023))*

## 潜在变量空间 (Latent Variable Space)

**潜在扩散模型** (**LDM**; [Rombach & Blattmann, et al. 2022](https://arxiv.org/abs/2112.10752)) 在潜在空间而不是像素空间运行扩散过程，从而降低了训练成本并加快了推理速度。其动机是观察到图像的大部分比特用于感知细节，而语义和概念构成在激进压缩后仍然保留。LDM 通过首先使用自动编码器修剪像素级冗余，然后在学习的潜在空间上使用扩散过程来操作/生成语义概念，从而松散地将感知压缩和语义压缩与生成建模分解开来。

![|500](Pasted%20image%2020260209180730.png)
*(图13: 压缩率与失真的权衡图，说明了两阶段压缩——感知和语义压缩。(图片来源: Rombach & Blattmann, et al. 2022))*

感知压缩依赖于自动编码器模型。编码器 $\mathcal{E}$ 用于将输入图像 $\mathbf{x} \in \mathbb{R}^{H \times W \times 3}$ 压缩为较小的 2D 潜在向量 $\mathbf{z} = \mathcal{E}(\mathbf{x}) \in \mathbb{R}^{h \times w \times c}$，其中下采样率 $f = H/h = W/w = 2^m, m \in \mathbb{N}$。然后解码器 $\mathcal{D}$ 从潜在向量重建图像，$\tilde{\mathbf{x}} = \mathcal{D}(\mathbf{z})$。论文探索了两种类型的正则化以避免潜在空间中的任意高方差：

*   *KL-reg:* 类似于 VAE，对学习到的潜在变量施加标准正态分布的微小 KL 惩罚。
*   *VQ-reg:* 在解码器中使用矢量量化层，如 VQVAE，但量化层被解码器吸收。

扩散和去噪过程发生在潜在向量 $\mathbf{z}$ 上。去噪模型是一个时间条件 U-Net，通过交叉注意力机制增强，以处理用于图像生成的灵活条件信息（例如类别标签、语义图、图像的模糊变体）。设计相当于将不同模态的表示融合到具有交叉注意力机制的模型中。每种类型的调节信息都与特定领域的编码器 $\tau_\theta$ 配对，以将调节输入 $y$ 投影到可以映射到交叉注意力组件的中间表示 $\tau_\theta(y) \in \mathbb{R}^{M \times d_\tau}$：

$$ \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\big( \frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}} \big) \cdot \mathbf{V} $$
$$ \text{where } \mathbf{Q} = \mathbf{W}_Q^{(i)} \cdot \varphi_i(\mathbf{z}_i), \; \mathbf{K} = \mathbf{W}_K^{(i)} \cdot \tau_\theta(y), \; \mathbf{V} = \mathbf{W}_V^{(i)} \cdot \tau_\theta(y) $$
$$ \text{and } \mathbf{W}_Q^{(i)} \in \mathbb{R}^{d \times d_\epsilon^i}, \; \mathbf{W}_K^{(i)}, \mathbf{W}_V^{(i)} \in \mathbb{R}^{d \times d_\tau}, \; \varphi_i(\mathbf{z}_i) \in \mathbb{R}^{N \times d_\epsilon^i}, \; \tau_\theta(y) \in \mathbb{R}^{M \times d_\tau} $$

![|500](Pasted%20image%2020260209180737.png)
*(图14: 潜在扩散模型 (LDM) 的架构。(图片来源: Rombach & Blattmann, et al. 2022))*

## 提升生成分辨率和质量 (Scale up Generation Resolution and Quality)

为了在高分辨率下生成高质量图像，[Ho et al. (2021)](https://arxiv.org/abs/2106.15282) 提出使用多个扩散模型在增加的分辨率下组成的流水线。*噪声调节增强 (Noise conditioning augmentation)* 对于流水线模型之间的最终图像质量至关重要，即对低分辨率调节输入应用强数据增强。

![](Pasted%20image%2020260209180746.png)
*(图15: 分辨率增加的级联扩散模型流水线 (图片来源: Ho et al. 2021))*

他们发现最有效的噪声是低分辨率下的高斯噪声和高分辨率下的高斯模糊。此外，他们还探索了两种形式的调节增强，这需要对训练过程进行微小的修改。注意，调节噪声仅用于训练，不用于推理。

*   *截断调节增强 (Truncated conditioning augmentation)* 在低分辨率下于步骤 $t > 0$ 早期停止扩散过程。
*   *非截断调节增强 (Non-truncated conditioning augmentation)* 运行完整的低分辨率反向过程直到步骤 0，然后通过 $\mathbf{z}_t \sim q(\mathbf{x}_t \vert \mathbf{x}_0)$ 破坏它，然后将破坏后的 $\mathbf{z}_t$ 馈送到超分辨率模型中。

两阶段扩散模型 **unCLIP** ([Ramesh et al. 2022](https://arxiv.org/abs/2204.06125)) 大量利用 CLIP 文本编码器生成高质量的文本引导图像。给定预训练的 CLIP 模型 $\mathbf{c}$ 和配对的训练数据用于扩散模型 $(\mathbf{x}, y)$，其中 $\mathbf{x}$ 是图像，$y$ 是相应的标题，我们可以计算 CLIP 文本和图像嵌入，$\mathbf{c}^t(y)$ 和 $\mathbf{c}^i(\mathbf{x})$。unCLIP 并行学习两个模型：

*   先验模型 $P(\mathbf{c}^i \vert y)$：给定文本 $y$ 输出 CLIP 图像嵌入 $\mathbf{c}^i$。
*   解码器 $P(\mathbf{x} \vert \mathbf{c}^i, [y])$：给定 CLIP 图像嵌入 $\mathbf{c}^i$ 和可选的原始文本 $y$ 生成图像 $\mathbf{x}$。

这两个模型使得条件生成成为可能，因为：

$$ P(\mathbf{x} \vert y) = P(\mathbf{x}, \mathbf{c}^i \vert y) = P(\mathbf{x} \vert \mathbf{c}^i, y)P(\mathbf{c}^i \vert y) $$


![](Pasted%20image%2020260209180754.png)
*(图16: unCLIP 的架构。(图片来源: Ramesh et al. 2022))*

unCLIP 遵循两阶段图像生成过程：
1.  给定文本 $y$，首先使用 CLIP 模型生成文本嵌入 $\mathbf{c}^t(y)$。使用 CLIP 潜在空间通过文本实现零样本图像处理。
2.  扩散或自回归先验 $P(\mathbf{c}^i \vert y)$ 预测 CLIP 图像嵌入以构建图像先验，然后扩散解码器 $P(\mathbf{x} \vert \mathbf{c}^i, [y])$ 生成图像。该解码器还可以生成以图像为条件的图像变体，保留其风格和语义。

代替 CLIP 模型，**Imagen** ([Saharia et al. 2022](https://arxiv.org/abs/2205.11487)) 使用预训练的大型 LM（即冻结的 T5-XXL 文本编码器）对文本进行编码以生成图像。有一个普遍趋势是，较大的模型尺寸可以带来更好的图像质量和文本-图像对齐。他们发现 T5-XXL 和 CLIP 文本编码器在 MS-COCO 上取得了相似的性能，但人类评估更偏向于 DrawBench（一组涵盖 11 个类别的提示）上的 T5-XXL。

当应用无分类器引导时，增加 $w$ 可能会导致更好的图像-文本对齐，但会损害图像保真度。他们发现这是由于训练-测试不匹配造成的，也就是说，因为训练数据 $\mathbf{x}$ 保持在范围 $[-1, 1]$ 内，测试数据也应该如此。因此引入了两种阈值策略：

*   *静态阈值 (Static thresholding):* 将 $\mathbf{x}$ 预测截断为 $[-1, 1]$。
*   *动态阈值 (Dynamic thresholding):* 在每个采样步骤中，将 $s$ 计算为某个百分位的绝对像素值；如果 $s > 1$，则将预测截断为 $[-s, s]$ 并除以 $s$。

Imagen 修改了 U-Net 中的几个设计以使其更高效 *Efficient U-Net*。
*   通过添加更多残差锁将模型参数从高分辨率块转移到低分辨率块；
*   将跳跃连接缩放 $1/\sqrt{2}$；
*   反转下采样（在卷积前移动）和上采样操作（在卷积后移动）的顺序，以提高前向传播的速度。

他们发现噪声条件增强、动态阈值和高效 U-Net 对图像质量至关重要，但缩放文本编码器大小比 U-Net 大小更重要。

## 模型架构 (Model Architecture)

扩散模型有两种常见的骨干架构选择：U-Net 和 Transformer。

**U-Net** ([Ronneberger, et al. 2015](https://arxiv.org/abs/1505.04597)) 由下采样堆栈和上采样堆栈组成。
*   *下采样:* 每一步包括重复应用两个 3x3 卷积（无填充卷积），每个卷积后跟一个 ReLU 和一个步长为 2 的 2x2 最大池化。在每个下采样步骤中，特征通道的数量加倍。
*   *上采样:* 每一步包括对特征图进行上采样，然后进行 2x2 卷积，每个卷积将特征通道数量减半。
*   *快捷连接 (Shortcuts):* 快捷连接导致上采样堆栈中的对应层与下采样堆栈中的层连接，并提供处理过程中必要的丢失的高分辨率特征。

![](Pasted%20image%2020260209180912.png)
*(图17: U-Net 架构。每个蓝色方块是一个多通道特征图，通道数表示在顶部，高度 x 宽度尺寸在左下侧表示。灰色箭头标记快捷连接。(图片来源: Ronneberger, 2015))*

为了使图像生成能够以额外的图像为条件进行构图，如 Canny 边缘、Hough 线、用户涂鸦、人体骨骼、分割图、深度和法线，**ControlNet** ([Zhang et al. 2023](https://arxiv.org/abs/2302.05543)) 通过将原始模型权重的可训练副本锁定到 U-Net 的每个编码器层中来引入架构更改。确切地说，给定一个神经网络块 $\mathcal{F}_\theta(\cdot)$，ControlNet 执行以下操作：
1.  首先，冻结原始块的原始参数 $\theta$；
2.  将其克隆为具有可训练参数 $\theta_c$ 的副本，以及一个附加的条件向量 $\mathbf{c}$；
3.  使用两个零卷积层，表示为 $\mathcal{Z}_{\theta_1}(\cdot; \cdot)$ 和 $\mathcal{Z}_{\theta_2}(\cdot; \cdot)$，它们是权重和偏差均初始化为零的 1x1 卷积层，以连接这两个块。零卷积保护这个骨干网在初始训练步骤中免受随机噪声梯度的影响。
4.  最终输出为：$\mathbf{y}_c = \mathcal{F}_\theta(\mathbf{x}) + \mathcal{Z}_{\theta_2}(\mathcal{F}_{\theta_c}(\mathbf{x} + \mathcal{Z}_{\theta_1}(\mathbf{c})))$

![](Pasted%20image%2020260209180918.png)
*(图18: ControlNet 架构。(图片来源: Zhang et al. 2023))*

**扩散 Transformer (DiT; [Peebles & Xie, 2023](https://arxiv.org/abs/2212.09748))** 用于在潜在图块（patches）上运行的扩散建模，使用与 [LDM (潜在扩散模型)](#latent-variable-space) 相同的设计。DiT 具有以下设置：
1.  将输入的潜在表示 $\mathbf{z}$ 作为 DiT 的输入。
2.  "Patchify" 将大小为 $I \times I \times C$ 的潜在变量分割为大小为 $p$ 的补丁序列，并将其转换为大小为 $(I/p)^2$ 的序列。
3.  然后这一系列标记经过 Transformer 块。他们正在探索三种不同的设计，以处理基于上下文信息的条件生成，如时间步 $t$ 或类标签 $c$。在三种设计中，*adaLN (Adaptive layer norm-Zero)* 效果最好，优于上下文调节和交叉注意力块。在这个设计中，缩放和位移参数 $\gamma$ 和 $\beta$ 是从 $t$ 和 $c$ 的嵌入向量回归得到的。维度缩放参数 $\alpha$ 也是回归得到的，并在 DiT 块内的任何残差连接之前立即应用。
4.  Transformer 解码器输出噪声预测和对角协方差预测。

![](Pasted%20image%2020260209180923.png)
*(图19: 扩散 Transformer (DiT) 架构。(图片来源: Peebles & Xie, 2023))*

Transformer 架构可以很容易地扩大规模，并且众所周知。这是 DiT 最大的好处之一，因为随着计算量的增加和更大的 DiT 模型，其性能也会提高，并且根据实验，其计算效率更高。

## 快速总结 (Quick Summary)

*   **优点:** 易于计算（Tractability）和灵活性是生成建模中两个相互冲突的目标。易于计算的模型可以通过解析进行评估并容易地拟合数据（例如通过高斯或拉普拉斯），但它们不能轻易地描述丰富的数据结构。灵活的模型可以拟合任意的数据结构，但评估、训练或从这些模型中采样通常很昂贵。扩散模型在易于计算和灵活性方面都是兼容的。
*   **缺点:** 扩散模型依赖于长时间的马尔可夫扩散步骤链来生成样本，因此在时间和计算方面可能非常昂贵。已经提出了新方法来使该过程快得多，但采样仍然比 GAN 慢。