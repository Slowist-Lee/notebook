# Transformer

GPT：
Generative：功能：生成式大语言模型 
Pre-trained：预训练
Transformer：【key】一种关键的机器学习模型

Transformer可以用于多种任务，text-voice, voice-text, text-image ...

Transformer要做的任务是，给定一定的输入（可以是音频、文本、视频），预测未来的输出，将结果展现为接下来不同文本片段的概率分布

## 总体

1. 切成 token，每个token编码（embedding）成一组向量（vector），其中 embedding后的vector之间的距离可以代表语义
2. 这些向量进入Attention模块，相互传递信息并更新
3. 处理后进入MLP-multilayer perception
4. 再进入Attention模块，不断重复
5. 最后给出概率分布


Attention要做什么：
例如：tower有多种意思，但embedding后是同一个向量。但通过attention机制，可以让the eiffel tower后的tower准确变成“塔”的意思

最后给出概率分布完全基于最后一层Attention

## Single Head Attention

要做的事情：修正：形容词修饰后的名词所对应的向量，例如

![|500](Pasted%20image%2020260203221322.png)

### Query

Query：当更新creature时，需要询问前面的向量，例如图中的前面是否有形容我的词汇。我们把这样的询问称为Query。

![|500](Pasted%20image%2020260203221618.png)

我们由Embedding后的向量得到Query，方法是乘上一个 $W_Q$ , 如下图。其中$W_Q$ 是模型参数，是从数据中学习得到的，其机制可以大概理解为：当embedding后的creature是名词，就映射到“寻找形容词”的Query上。

![|500](Pasted%20image%2020260203221701.png)

### Key

我们还会让embedding后的向量乘$W_k$，得到Keys。可以理解为Answering the Queries. 

![](Pasted%20image%2020260203222444.png)

$W_k$也是将原来的向量降维了，当 $Q$ 和 $K$ 类似的时候，就说他们相匹配了

![](Pasted%20image%2020260203222705.png)

每一对都需要点积来计算相似度。点积越大相似越高，可以说 The embeddings of fluffy and blue attend to the embedding of creature

![](Pasted%20image%2020260203222745.png)

我们想根据这个来得到最终修正后的样子，需要对前面的这些词进行加权求和。这样这些分数不可以是$[-\infty, \infty]$，而应该是$[0,1]$。所以我们对每一列进行softmax

在Attention表达式中用很简洁的方法表示了这个过程，即$QK^T$,用来表示这个点积的表格

![|500](Pasted%20image%2020260203223416.png)

其中 $d_k$ 是 $Q, K$ 的维度, $\frac{1}{\sqrt{d_k}}$ 是为了梯度爆炸(当 $Q$ 和 $K$ 进行点积运算 ($QK^T$) 时，如果向量的维度 $d_k$ 很大，点积的结果往往会变得非常大)

实际训练中，会根据预测结果对权重进行微调（奖惩）。已给的内容里可以根据token多进行几次预测，就会多一些训练的机会

![|500](Pasted%20image%2020260203224852.png)

我们不希望后面的词会影响前面的词，因此在softmax之前我们将他们设成$-\infty$，softmax后设成0. 这种方法叫masking

$QK^T$的大小是context length的平方，这意味着这个矩阵变得很大，因此出现了很多技术来解决这个问题

### Value

要解决的问题：将原本的creature向量移动到fluffy creature的位置 —— 使用 Value Matrix，可以理解为creature向量 + $W_v$ 向量 = fluffy creature 向量

在变化的时候，需要对 $\overrightarrow{V_i}$ 做加权求和，来移到对应的位置 $\Rightarrow \Delta E$.  因此得到 $\overrightarrow{E_4}+ \Delta \overrightarrow{E_4} = \overrightarrow{E_4'}$

![|500](Pasted%20image%2020260204104315.png)

![|500](Pasted%20image%2020260204104515.png)

 根据上面的描述，我们能发现整个Attention模块是由三个参数矩阵来实现的：Query, Key, Value. 而Value矩阵的尺寸为$d\_input \times d\_output$，输入输出都存在于embedding space

 ![|575](Pasted%20image%2020260204104707.png)

 ![|375](Pasted%20image%2020260204104831.png)

 常用的做法会让 \# of value = \# of Query + \# of Key，在Multi-headed attention里经常用到，因此会将Value矩阵视为两个小矩阵相乘。其中第一个矩阵相当于把value降维到128的空间，第二个相当于重新升维。我们称为 $Value_{\downarrow}$ 和 $Value_{\uparrow}$ 。这个操作的实质是对矩阵进行 **低秩分解**

 ![|575](Pasted%20image%2020260204111219.png)

 这四个矩阵的规模是一样的。因此，总参数量：

 ![|500](Pasted%20image%2020260204111332.png)

## Cross Attention

用途：类似 机翻/audio to text

![](Pasted%20image%2020260204111539.png)

以翻译任务为例，Cross attention的主要区别是Key和Query会来自两种语言

![|550](Pasted%20image%2020260204111644.png)


这种情况不需要masking，不存在后面的词会影响前面的情况。

## 如何重复注意力模块 = multihead

由于更新词义的方式肯定是有区别的，事实上GPT-3内有96种查询方式，也就有96个$W_K^{(i)},W_Q^{(i)}$

![](Pasted%20image%2020260204112237.png)


![](Pasted%20image%2020260204112328.png)

在实际实现中 $\text{Value}_{\uparrow}$ 矩阵会被合起来被称作 **输出矩阵**，而 只有这里的 $\text{Value}_{\downarrow}$ 才会被称作 单个注意力头的值矩阵

![](Pasted%20image%2020260204112440.png)

## MLP

MLP是非线性的，Attention模块内部实质上还是矩阵相乘、线性变换。其实这个MLP模块也可以理解为MLP增加了表达能力，但3B1B还是做了进一步解释：

首先，这个模块要做的就是给一个偏置的向量（例如：basketball），从而加到原始的Michael Jordan这个向量上

![](Pasted%20image%2020260204114710.png)

MLP只对某个向量进行处理，向量之间不存在互相影响。向量之间是并行处理的。

### Step 1

使用一个充满参数的矩阵（MLP第一个线性层的权重矩阵），乘上embedding后的向量，再加上Bias。即：
$$W_{\uparrow} E_i + B_{\uparrow}$$
![](Pasted%20image%2020260204125719.png)

![](Pasted%20image%2020260204125715.png)


![](Pasted%20image%2020260204124135.png)

### Step 2

Non-Linear function，激活函数，我们使用 ReLU。Neurons of transformers指的就是ReLU后的这一些

![](Pasted%20image%2020260204125700.png)

### Step 3

依旧是一个Linear层，只不过这次是降维，$W_{\downarrow} E_i + B_{\downarrow}$ 可以使用列视角，将neurons视作0/1的开关，决定加不加这一列

![](Pasted%20image%2020260204125614.png)


## Transformers 架构

参考： https://zhuanlan.zhihu.com/p/338817680

整体结构：

![|475](Pasted%20image%2020260207121654.png)

![](Pasted%20image%2020260207122828.png)

单词本身Embedding+位置Embedding

![](Pasted%20image%2020260207121736.png)

**因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。**所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。

位置 Embedding 用 **PE**表示，**PE** 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：
![|350](Pasted%20image%2020260207122604.png)

这可以让模型容易地计算出相对位置，对于固定长度的间距 k，**PE(pos+k)** 可以用 **PE(pos)** 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。

![](Pasted%20image%2020260207122941.png)

## ViT

参考： https://zhuanlan.zhihu.com/p/427388113

![](Pasted%20image%2020260207174936.png)

Transformer原本是用来做NLP的工作的，所以ViT的首要任务是将图转换成词的结构，这里采取的方法是如上图左下角所示，将图片分割成小块，每个小块就相当于句子里的一个词。这里把每个小块称作Patch，而**Patch Embedding**就是把每个Patch再经过一个全连接网络压缩成一定维度的向量。
  

这里是**VisionTransformer**源代码中关于Patch Embedding的部分：

```python
# 默认img_size=224, patch_size=16，in_chans=3，embed_dim=768，
self.patch_embed = embed_layer(
    img_size=img_size, patch_size=patch_size, 
    in_chans=in_chans, embed_dim=embed_dim)
```

## ViT 调参

小型 ViT 容易过拟合，所以把`weight_decay`调大一些

试了一下，patch_size 8 比 4的效果好（不知道为什么，瞎调的

试一下 weight_decay 0.05 比 0.1效果好 

batch_size 128 比 64有微小进步，256又退步了

dim_forward 128居然是最好的

weight_decay降到0.0001，完美！
