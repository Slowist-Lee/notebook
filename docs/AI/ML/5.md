# 自动微分框架实现

> [!warning]
> 本笔记完全由Gemini-2.5 Pro生成。

## Part 1: 环境准备与代码库设置

在 Google Colab 环境中把 Needle 框架的代码准备好。

### 1.1 挂载 Google Drive 并克隆代码库

首先，我们需要运行一段代码来设置工作环境。这段代码会将课程的代码库克隆到你的 Google Drive 中，并建立一个符号链接，方便我们在 Colab 中直接访问。

> [!EXAMPLE] 设置代码库
> ```python
> # 运行这段代码来设置环境
> from google.colab import drive
> drive.mount('/content/drive')
> %cd /content/drive/MyDrive/
> !mkdir -p 10714f24
> %cd /content/drive/MyDrive/10714f24
> # 注意：如果你是第一次运行，请取消下面这行代码的注释
> # !git clone https://github.com/dlsyscourse/lecture5
> !ln -s /content/drive/MyDrive/10714f24/lecture5 /content/needle
> ```
> **代码解释:**
> 1.  `drive.mount`: 授权并挂载你的 Google Drive 到 Colab 的 `/content/drive` 目录。
> 2.  `%cd`: 切换当前工作目录到你的 Google Drive。
> 3.  `!mkdir`: 创建一个用于存放课程代码的文件夹。
> 4.  `!git clone`: 从 GitHub 上克隆代码库。**如果你已经克隆过了，记得注释掉这行，否则会报错。** 这样做的好处是，你对代码的任何修改都会保存在你的 Drive 中，下次打开 Colab 不会丢失。
> 5.  `!ln -s`: 创建一个名为 `needle` 的快捷方式（符号链接），指向我们刚刚克隆的文件夹。这样可以简化后续的路径引用。

### 1.2 设置 `PYTHONPATH`

为了让 Python 能够找到 `needle` 这个库，我们需要把它所在的路径添加到环境变量 `PYTHONPATH` 中。

```python
%set_env PYTHONPATH /content/needle/python:/env/python
import sys
sys.path.append("/content/needle/python")
```

执行完以上步骤后，你的 Colab 环境左侧的文件浏览器中应该能看到一个 `needle` 文件夹。它的结构大致如下：

-   `needle/python/needle`
    -   `__init__.py`: 包的初始化文件。
    -   `autograd.py`: **核心文件！** 定义了自动微分机制和核心数据结构。
    -   `ops/`: 存放所有计算操作（如加法、乘法、指数等）的定义。

---

## Part 2: Tensor 的初体验

现在，环境搭好了，我们来实际体验一下 `needle` 框架。它的使用感受和 PyTorch 非常相似。

> [!EXAMPLE] 创建和操作 Tensor
> ```python
> import needle as ndl
>
> # 1. 创建一个 Needle Tensor
> x = ndl.Tensor([1, 2, 3], dtype="float32")
> print(x)
> # 输出: needle.Tensor([1. 2. 3.])
>
> # 2. 在 Tensor 上执行计算
> y = x + 1
> print(y)
> # 输出: needle.Tensor([2. 3. 4.])
> ```
>
> `y = x + 1` 这种写法非常直观，它背后其实是**运算符重载 (Operator Overloading)**。当你执行这个操作时，`needle` 框架会自动调用内部定义的 `add_scalar` 函数：
>
> ```python
> # 上面的 y = x + 1 等价于：
> y = ndl.add_scalar(x, 1)
> ```

### 2.1 将 Needle Tensor 转换回 NumPy

为了方便验证和调试，我们可以随时将 `needle` 的 Tensor 对象转换回我们熟悉的 NumPy 数组。

```python
# 使用 .numpy() 方法转换
numpy_array = y.numpy()
print(numpy_array)
# 输出: array([2., 3., 4.], dtype=float32)

print(type(numpy_array))
# 输出: <class 'numpy.ndarray'>
```

> [!TIP] 为什么需要 `.numpy()` 方法？
> 你可能会想，既然现在的底层就是用 NumPy 实现的，为什么还要多此一举？
> 讲座中解释道：这是为了**框架未来的扩展性**。在课程的后续部分，我们会用自己编写的、能在 CPU 和 GPU 上运行的 `NDArray` 来替换 NumPy。到那时，数据可能存储在 GPU 显存中，就不能直接访问了。
>
> `.numpy()` 这个 API 保证了无论底层实现如何，我们总有一种统一的方式将数据拷贝回 CPU 上的 NumPy 数组中。因此，在编写测试用例时，总是调用 `.numpy()` 来获取结果是一个好习惯。

---

## Part 3: 核心数据结构：构建计算图

深度学习框架的核心魔法在于**自动微分**，而自动微分的基础是**计算图 (Computational Graph)**。`needle` 中的 `Tensor` 不仅仅是一个存数据的东西，它还是计算图中的一个节点，记录了自己是如何被计算出来的。

![|200](Pasted%20image%2020251209221905.png)

### 3.1 核心类：`Value` 和 `Op`

`needle` 通过两个核心基类来构建计算图：

1.  **`Value` 类 (`autograd.py`)**: 代表计算图中的一个**节点 (Node)**。我们使用的 `Tensor` 就是 `Value` 的一个子类。
2.  **`Op` 类 (`ops.py`)**: 代表计算图中的一个**操作 (Operation)**，比如加法、乘法。

> [!NOTE] `Value` 类的核心属性
> 一个 `Value` (或 `Tensor`) 对象包含了以下关键信息，用于追溯它的“身世”：
>
> -   `cached_data`: 缓存该节点的计算结果。它是一个底层的数组对象（在我们的例子中是 `numpy.ndarray`）。
> -   `op`: 指向一个 `Op` 对象，表示这个节点是**通过什么操作**计算得来的。
> -   `inputs`: 一个元组 (tuple)，包含了所有**输入给 `op`** 的父节点 (`Value` 对象)。
>
> **举个例子**: 对于计算 `v4 = v2 * v3`：
> -   `v4` 是一个 `Value` 对象。
> -   `v4.op` 是一个 `EWiseMul` (逐元素乘法) 的 `Op` 对象。
> -   `v4.inputs` 是一个包含 `v2` 和 `v3` 两个 `Value` 对象的元组。
>
> 通过 `inputs` 属性，我们可以从任何一个节点出发，像链表一样向前回溯，直到找到所有最初的输入（我们称之为**叶子节点**）。叶子节点的 `op` 属性为 `None`。

### 3.2 动手构建与探索计算图

让我们用代码来构建上图所示的计算图，并亲自探索它的内部结构。

> [!EXAMPLE] 构建并检查计算图
> ```python
> # 切换回 eager 模式，方便观察
> ndl.autograd.LAZY_MODE = False
>
> v1 = ndl.Tensor([0], dtype="float32")
> v2 = ndl.exp(v1)
> v3 = v2 + 1
> v4 = v2 * v3
>
> print(f"v4 的值: {v4.numpy()}")
> # 输出: v4 的值: [2.]
>
> # 检查 v4 的输入
> print(f"v4 的输入节点是 v2 和 v3 吗? {v4.inputs[0] is v2 and v4.inputs[1] is v3}")
> # 输出: v4 的输入节点是 v2 和 v3 吗? True
>
> # 检查 v4 的操作
> print(f"v4 的操作是: {v4.op}")
> # 输出: v4 的操作是: <needle.ops.ops_mathematic.EWiseMul object at ...>
>
> # 我们可以一路追溯到 v1
> print(f"v4 -> v2 -> v1 链路是否正确? {v4.inputs[0].inputs[0] is v1}")
> # 输出: v4 -> v2 -> v1 链路是否正确? True
> ```
>
> 为了更清晰地观察节点信息，我们可以定义一个辅助函数 `print_node`，它会打印出每个节点的唯一 ID、它的操作和输入节点的 ID。
>
> ```python
> def print_node(node):
>     print(f"ID:          {id(node)}")
>     print(f"Op:          {node.op}")
>     print(f"Input IDs:   {[id(x) for x in node.inputs]}")
>     print(f"Cached Data: {node.cached_data}\n")
>
> print("--- v4 的信息 ---")
> print_node(v4)
>
> print("--- v3 的信息 ---")
> print_node(v3)
>
> print("--- v2 的信息 ---")
> print_node(v2)
> ```
> 运行后，你会发现 `v4` 的 `Input IDs` 列表中的两个 ID 正好对应 `v2` 和 `v3` 的 ID，这清晰地证明了计算图的连接关系。

---

## Part 4: 计算的执行：Eager vs. Lazy 模式

> [!QUESTION] 计算是什么时候发生的？
> 当我们写下 `y = x1 + x2` 时，加法计算是立刻执行了，还是仅仅构建了图？

这个问题的答案取决于框架的**执行模式**。`needle` 支持两种模式：

1.  **Eager Mode (即时模式/动态图)**: **默认模式**。每执行一个操作，就**立即计算**结果并存入 `cached_data`。PyTorch 默认也是这种模式。
    *   **优点**：非常直观，便于调试，可以随时打印任何中间变量的值。
2.  **Lazy Mode (延迟模式/静态图)**: 当执行操作时，只构建计算图，并**不进行实际计算** (`cached_data` 此时为 `None`)。只有当用户真正需要结果时（例如调用 `.numpy()`），框架才会从该节点回溯，执行整个子图的计算。TensorFlow 的早期版本就是这种模式。
    *   **优点**：框架可以先拿到完整的计算图，然后对整个图进行优化（比如算子融合），这对于后端编译（如在 TPU 上运行）非常有利。

> [!EXAMPLE] 体验 Lazy Mode
> ```python
> # 开启 Lazy Mode
> ndl.autograd.LAZY_MODE = True
>
> x1 = ndl.Tensor([3], dtype="float32")
> x2 = ndl.Tensor([4], dtype="float32")
> x3 = x1 * x2
>
> # 此时，计算图已经构建，但 x3 的值还没计算
> print(f"x3 的 cached_data 是 None 吗? {x3.cached_data is None}")
> # 输出: x3 的 cached_data 是 None 吗? True
>
> # 当我们请求具体数值时，才会触发计算
> print(f"请求 x3.numpy(): {x3.numpy()}")
> # 输出: 请求 x3.numpy(): [12.]
>
> # 再次检查，发现 cached_data 已经被填充了
> print(f"x3 的 cached_data 现在是: {x3.cached_data}")
> # 输出: x3 的 cached_data 现在是: [12.]
> ```
>
> 这一切背后的“魔法”是 `realize_cached_data()` 方法。当你需要数据时，它会**递归地**先去计算所有输入节点的 `cached_data`，然后再调用当前节点的 `op.compute()` 方法来计算自己的值。


## Part 5: 一个常见的陷阱：计算图与内存管理

由于 `Tensor` 会记录计算历史，如果不加注意，可能会导致计算图无限增长，最终耗尽内存。

> [!WARNING] 内存泄漏陷阱
> 想象一下在训练循环中累加损失 (loss)：
>
> ```python
> x = ndl.Tensor([2], dtype="float32")
> sum_loss = ndl.Tensor([0], dtype="float32")
>
> for i in range(100):
>     # 每次循环，sum_loss 都会把之前的计算历史连接起来
>     sum_loss = sum_loss + x * x
>
> # 循环结束后，sum_loss 的计算图会非常长，像这样：
> # ((((0 + x*x)_0 + x*x)_1 + x*x)_2 ... + x*x)_99
> # 这会占用大量内存！
> ```
> 在 PyTorch 中，这是新手常犯的错误之一。我们只是想累加一个数值，但却无意中保留了整个迭代过程的计算历史。

### 5.1 解决方案：`.detach()` 方法

为了解决这个问题，我们需要在不需要反向传播的时候**切断计算图**。`.detach()` 方法就是为此而生。

-   `.detach()` 会创建一个**新的 `Tensor`**。
-   这个新的 `Tensor` 和原来的 `Tensor` 共享相同的 `cached_data` (数值相同)。
-   但是，新的 `Tensor` **不包含任何计算历史** (它的 `op` 和 `inputs` 都是 `None`)，它是一个新的叶子节点。

> [!EXAMPLE] 使用 `.detach()` 修复内存问题
> ```python
> x = ndl.Tensor([3], dtype="float32")
> sum_loss = ndl.Tensor([0], dtype="float32")
>
> for i in range(100):
>     # 计算完当前步的 loss 后，立即 detach()
>     # 这样 sum_loss 就只是一个数值，不再有计算历史
>     sum_loss = (sum_loss + x * x).detach()
>
> # 检查最终的 sum_loss，它的输入为空，计算图被切断了
> print(sum_loss.inputs)
> # 输出: []
> ```


## Part 6: 自动微分的实现机制

终于到了最核心的部分——自动微分。`needle` 框架是如何实现反向传播的呢？

关键在于每个 `Op` 对象都必须实现的 `gradient` 方法。

> [!NOTE] `gradient` 方法的职责
> `gradient` 方法定义了**反向传播的局部规则**。它接收两个参数：
>
> 1.  `out_grad`: 输出节点（也就是当前 `Op` 的结果节点）的梯度，它本身也是一个 `Tensor`。
> 2.  `node`: 当前 `Op` 的结果节点自身（包含Inputs/Op...）。
>
> 它需要返回一个元组，其中包含了 `out_grad` 传播到**每一个输入节点**的梯度值（偏导数）。
>
> 以乘法 `v4 = v2 * v3` 为例，根据链式法则：
> -   `v2` 的梯度是：$\frac{\partial L}{\partial v_2} = \frac{\partial L}{\partial v_4} \cdot \frac{\partial v_4}{\partial v_2} = \bar{v_4} \cdot v_3$
> -   `v3` 的梯度是：$\frac{\partial L}{\partial v_3} = \frac{\partial L}{\partial v_4} \cdot \frac{\partial v_4}{\partial v_3} = \bar{v_4} \cdot v_2$
>
> 其中，$\bar{v_4}$ 就是 `out_grad`。因此，`EWiseMul` 操作的 `gradient` 方法就需要返回 `(out_grad * v3, out_grad * v2)`。

### 6.1 `gradient` 函数返回的是 Tensor

> [!TIP] 为什么 `gradient` 函数返回的也是 `Tensor`？
> 这是一个非常精妙的设计。`gradient` 函数内部的计算（如 `out_grad * v3`）也是通过 `needle` 的操作来完成的。这意味着，在执行反向传播时，我们实际上是在**构建一个新的计算图——梯度的计算图**。
>
> 这样做最大的好处是，我们可以对梯度再次求导，也就是实现**高阶微分 (gradient-of-gradient)**。

> [!EXAMPLE] 单步梯度计算
> 让我们手动调用 `v4` 所在操作的 `gradient` 方法，看看它的输出。
>
> ```python
> # 假设 v4 的梯度为 1
> v4_adj = ndl.Tensor([1], dtype="float32")
>
> # 调用 v4.op 的 gradient 方法
> # v4.op 是 EWiseMul，它会返回一个包含两个梯度的元组
> v2_adj, v3_adj = v4.op.gradient_as_tuple(v4_adj, v4)
>
> print(f"v2 的梯度: {v2_adj.numpy()}") # 应该等于 v4_adj * v3 = 1 * 2 = 2
> print(f"v3 的梯度: {v3_adj.numpy()}") # 应该等于 v4_adj * v2 = 1 * 1 = 1
>
> # 检查 v2_adj，它自己也是一个计算图节点！
> print("\n--- v2_adj 的信息 ---")
> print_node(v2_adj)
> ```
>
> `v4.backward()` 的完整实现（这是你们作业的一部分）就需要：
> 1.  对整个计算图进行**拓扑排序**。
> 2.  从输出节点开始**反向遍历**这个排序。
> 3.  在每个节点上，调用其 `op.gradient` 方法计算出梯度，并将这些梯度**累加**到对应的输入节点的 `.grad` 属性上。

## 总结

通过这节课的实践，我们深入了解了一个深度学习框架的核心组件：

-   **`Tensor`** 不仅是数据容器，更是**计算图**中的节点，记录着计算历史。
-   框架可以通过**即时 (Eager)** 或**延迟 (Lazy)** 模式执行计算，各有优劣。
-   使用 `.detach()` 可以有效**管理内存**，避免计算图无限增长。
-   **自动微分**的核心是 `Op` 中的 `gradient` 方法，它定义了链式法则的局部规则。
-   通过将梯度也表示为 `Tensor`，框架能够构建**梯度的计算图**，从而支持高阶微分。
