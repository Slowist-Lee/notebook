好的，这页PPT是对上一页提到的 **Xavier 初始化**（也叫 Glorot 初始化）背后原理的数学推导。我们用一个 Callout 笔记来把它彻底讲清楚。

---

> [!abstract] 深入理解 Xavier 初始化：从理论到实践
>
> 上一页我们讲了，一个好的初始化策略是让神经元的输入加权和 $z$ 保持在 0 附近。Xavier 初始化提供了一个更精确的目标：**保持每一层激活值（输出）的方差不变**。
>
> **核心思想：** 如果信息（在这里用方差来衡量）在网络中向前传播时，每经过一层，其“强度”既不急剧放大，也不急剧衰减，那么信号就能更稳定地流动，梯度也能更稳定地反向传播。
>
> ---
>
> ### 1. 目标与方法
>
> *   **最终目标 (The Aim):**
>     让第 $l$ 层的激活值 $a^{(l)}$ 的方差，等于它上一层（第 $l-1$ 层）的激活值 $a^{(l-1)}$ 的方差。用数学语言描述就是：
>     $$
>     Var(a^{(l)}) = Var(a^{(l-1)})
>     $$
>
> *   **实现方法 (The Setting):**
>     为了达到这个目标，Xavier 初始化提出如下策略：
>     *   将权重 $W_{ij}^{(l)}$ 从一个**均值为0，方差为 $\frac{1}{n^{(l-1)}}$ 的正态分布**中采样。$n^{(l-1)}$ 是输入神经元的数量（即上一层的神经元个数）。
>       $$
>       W_{ij}^{(l)} \in N(0, \frac{1}{n^{(l-1)}})
>       $$
>     *   将偏置 $b^{(l)}$ 初始化为 0。
>
> ---

### 2. 数学推导 (The Deduction)

下面我们来一步步看，这个初始化方法是如何实现我们的目标的。整个推导建立在一些合理的**假设**之上：
*   权重 $W$ 和输入 $a$ 相互独立。
*   权重 $W$ 和输入 $a$ 的均值都为0。
*   我们使用的激活函数（如 tanh）在0附近近似于线性函数，即 $\varphi(z) \approx z$。

**推导步骤：**

1.  **从目标出发：**
    $Var(a_i^{(l)})$

2.  **利用线性假设：** 因为我们想让 $z$ 在0附近，所以激活函数 $\varphi(z_i^{(l)})$ 近似等于 $z_i^{(l)}$。因此它们的方差也近似相等。
    $= Var(z_i^{(l)})$
    > 📌 **注释:** `linearity of tanh around zero, tanh(z) ≈ z`

3.  **代入 $z$ 的定义：** $z_i^{(l)} = \sum_{j=1}^{n^{(l-1)}} w_{ij}^{(l)} a_j^{(l-1)}$ （这里我们暂时忽略偏置 $b$，因为它被初始化为0）。
    $= Var(\sum_{j=1}^{n^{(l-1)}} w_{ij}^{(l)} a_j^{(l-1)})$

4.  **利用方差的可加性：** 如果各个项是相互独立的，那么“和的方差”等于“方差的和”。
    $= \sum_{j=1}^{n^{(l-1)}} Var(w_{ij}^{(l)} a_j^{(l-1)})$
    > 📌 **注释:** `variance of independent sum, Var(X+Y) = Var(X) + Var(Y)`

5.  **利用方差的乘法公式：** 对于两个均值为0的独立随机变量 $X, Y$，$Var(XY) = Var(X)Var(Y)$。
    $= \sum_{j=1}^{n^{(l-1)}} Var(w_{ij}^{(l)}) Var(a_j^{(l-1)})$
    > 📌 **注释:** `variance of independent product` (这里使用了简化后的公式)

6.  **简化求和：** 假设所有 $w_{ij}^{(l)}$ 的方差都相同，记为 $Var(W^{(l)})$；所有 $a_j^{(l-1)}$ 的方差都相同，记为 $Var(a^{(l-1)})$。那么求和就变成了 $n^{(l-1)}$ 次的相同项相加。
    $= n^{(l-1)} Var(W^{(l)}) Var(a^{(l-1)})$

**推导结论：**

我们得到了关系式：
$$
Var(a^{(l)}) = n^{(l-1)} \cdot Var(W^{(l)}) \cdot Var(a^{(l-1)})
$$

为了实现我们的最初目标 $Var(a^{(l)}) = Var(a^{(l-1)})$，我们只需要让中间的乘数项等于1即可：
$$
n^{(l-1)} \cdot Var(W^{(l)}) = 1
$$
移项后，我们就得到了权重的方差应该满足的条件：
$$
Var(W^{(l)}) = \frac{1}{n^{(l-1)}}
$$
这就是Xavier初始化方法的理论依。

---

### 3. 代码实现 (Python Code)

*   **设置偏置为0:**
    `nn.b{k} = zeros(height, 1);`

*   **设置权重:**
    `nn.w{k} = 2*randn(height, width)/sqrt(width);`

    **代码解析：**
    *   这里的 `width` 就是我们公式中的 $n^{(l-1)}$（输入维度）。
    *   `randn(height, width)` 生成一个服从**标准正态分布**（均值为0，方差为1）的矩阵。
    *   要使一个随机变量的方差从1变为 $V$，需要将它乘以 $\sqrt{V}$。
    *   因此，为了让权重方差为 $\frac{1}{\text{width}}$，我们需要将 `randn` 的结果乘以 $\sqrt{\frac{1}{\text{width}}} = \frac{1}{\sqrt{\text{width}}}$。
    *   所以标准的 Xavier 正态初始化代码应该是 `randn(height, width) / sqrt(width)`。PPT中的 `2*` 可能是一个笔误，或者是针对不同激活函数（如ReLU）的变体（Kaiming He 初始化）的混淆。标准的Xavier初始化不包含这个系数2。

