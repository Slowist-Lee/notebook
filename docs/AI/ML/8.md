# Transformer & Attention

## 1. 时间序列预测基础

回顾之前课程中的基本时间序列预测任务：
给定输入序列 $x_1, x_2, x_3, \dots$，目标是预测输出序列 $y_1, y_2, y_3, \dots$。

从根本上讲，时间序列预测任务是预测函数 $f_\theta$：
$$ y_{1:T} = f_{\theta}(x_{1:T}) $$
其中，$y_t$ **只能依赖于** $x_{1:t}$（即当前的预测只能利用当前及过去的信息，不能利用未来的信息）。

> [!info] 说明
> 实现这一目标有多种方法，这些方法可能涉及也可能不涉及RNN中的“潜在状态（latent state）”表示。

## 3. 方法一：RNN "潜在状态" 方法
*(对应第 5 页)*

### 核心思想
RNN（循环神经网络）通过维护一个“潜在状态” $h_t$ 来总结直到当前时刻的所有信息。
$$ h_t = f(x_t, h_{t-1}) $$
$$ y_t = g(h_t) $$

![[Placeholder: Slide 5 RNN structure diagram]]

### 优缺点分析
*   **优点 (Pros)**:
    *   理论上拥有“无限”的历史记忆（Potentially "infinite" history）。
    *   表示形式非常紧凑（compact representation）。
*   **缺点 (Cons)**:
    *   历史信息与当前时刻之间的“计算路径”很长（Long "compute path"）。
    *   导致梯度消失或梯度爆炸问题（vanishing / exploding gradients）。
    *   难以训练（hard to learn）。

---

## 4. 方法二："直接预测" 方法
*(对应第 6 页)*

### 核心思想
与RNN不同，直接预测方法不需要维护隐状态，而是直接根据过去的输入预测输出：
$$ y_t = f_{\theta}(x_{1:t}) $$
只需要一个能够处理不同长度输入的函数即可。

![[Placeholder: Slide 6 Direct prediction diagram]]

### 优缺点分析
*   **优点 (Pros)**:
    *   通常从过去到当前状态的映射具有更短的计算路径（shorter compute path）。
*   **缺点 (Cons)**:
    *   没有紧凑的状态表示（No compact state representation）。
    *   在实践中通常只能利用有限的历史信息（finite history）。

---

## 5. 用于直接预测的 CNN (TCNs)


### 实现方式
定义函数 $f_\theta$ 最直接的方法之一是使用**(全)卷积网络**（(fully) convolutional networks），也称为**时间卷积网络 (Temporal Convolutional Networks, TCNs)**。

### 约束条件
主要的约束是卷积必须是**因果的 (causal)**：
即 $z_t^{(i+1)}$ 只能依赖于 $z_{t-k:t}^{(i)}$（不能看到未来的数据）。

### 应用案例
*   **WaveNet** (van den Oord et al., 2016)：用于语音生成的成功案例。

![[Placeholder: Slide 7 Dilated Causal Convolution diagram]]

---

## 6. CNN 进行密集预测的挑战
*(对应第 8 页)*

### 问题：感受野 (Receptive Field)
尽管 CNN 结构简单，但在时间序列预测中有一个显著缺点：
每个卷积核的**感受野 (receptive field)** 通常相对较小 $\implies$ 需要非常深的网络才能真正整合过去较远的信息。

### 潜在解决方案
1.  **增加卷积核大小 (Increase kernel size)**:
    *   代价：会增加网络的参数量。
2.  **池化层 (Pooling layers)**:
    *   问题：不太适合密集预测（dense prediction），因为我们需要预测每一个 $y_{1:T}$，池化会丢失分辨率。
3.  **空洞卷积 (Dilated convolutions)**:
    *   原理：在卷积中“跳过”一些过去的状态/输入，从而在不增加参数的情况下扩大感受野。

## 7. 深度学习中的 "Attention" (注意力机制)

> [!note] 定义
> 深度网络中的注意力（"Attention"） 通常指一种机制，它给每一个独立的状态（比如句子中每个单词生成的特征向量）分配一个**权重（weight）**，然后把它们**组合（加权求和）** 在一起。这就好比你在读一句话时，你的注意力会更多地放在几个关键词上，而不是平均分配给每个字。

数学原理如下：
$$ z_t = \theta^T h_t^{(k)} \quad \quad \text{— 计算分数} \, z_t $$
$$ w = \text{softmax}(z)  \quad \text{— 计算权重} \, \omega$$
$$ \bar{h} = \sum_{t=1}^T w_t h_t^{(k)} \quad \text{— 加权求和得到上下文向量} \, \overline{h} $$

![|350](Pasted%20image%2020260202153218.png)

这样的方法最初用于 RNN 中，目的是以一种比“仅查看最后一个状态”更通用的方式，将所有时刻的潜在状态组合起来。

## 8. Self-Attention (自注意力) 操作

Self-attention 是一种特定形式的注意力机制。
给定三个输入 $K, Q, V \in \mathbb{R}^{T \times d}$（分别代表 "Keys"【索引/标签】, "Queries"【查询请求，我当前想找什么】, "Values"【标签对应的内容】）。矩阵形式如下：
$$ K = \begin{bmatrix} - k_1^T - \\ \vdots \\ - k_T^T - \end{bmatrix}, \quad Q = \begin{bmatrix} - q_1^T - \\ \vdots \\ - q_T^T - \end{bmatrix}, \quad V = \begin{bmatrix} - v_1^T - \\ \vdots \\ - v_T^T - \end{bmatrix} $$

我们定义 Self-Attention 操作为：
$$ \text{SelfAttention}(K, Q, V) = \text{softmax}\left( \frac{KQ^T}{d^{1/2}} \right) V $$
## 9. Self-Attention 详解

### 结构分析
$$ \text{softmax}\underbrace{\left( \frac{KQ^T}{d^{1/2}} \right)}_{T \times T \text{ "weight" matrix}} V $$
*   Softmax 是**逐行 (rowwise)** 应用的。
*   中间生成了一个 $T \times T$ 的权重矩阵，表示不同时间步之间的关联强度。

### Self-attention 的性质
1.  **置换等变性 (Permutation Equivariant)**:
    *   对于 $K, Q, V$ 矩阵的顺序变换是不敏感的（即它本身不包含序列的顺序信息，打乱输入顺序，输出对应的位置也会相应打乱，但值不变）。
2.  **全局影响 (Global Influence)**:
    *   允许 $k_t, q_t, v_t$ 在**所有**时间步之间产生影响（全连接的图结构）。
3.  **计算成本 (Compute Cost)**:
    *   复杂度为 $O(T^2 + Td)$。
    *   由于非线性操作应用在完整的 $T \times T$ 矩阵上，这部分计算量很难简化。
## 10. 用于时间序列的 Transformers


### 架构概览
Transformer 架构使用一系列的 Attention 机制（和前馈层）来处理时间序列。
$$ Z^{(i+1)} = \text{Transformer}(Z^{(i)}) $$

### 特点
*   所有的计算步骤（在实践中，指在一个给定的时间切片内）都是**并行处理 (processed in parallel)** 的。
*   避免了像 RNN 那样的顺序处理需求。

---

## 11. Transformer Block (模块详情)

### 具体公式
Transformer 块通常具有以下形式（包含残差连接和归一化）：

1.  **Attention 层**:
    $$ \tilde{Z} := \text{SelfAttention}(Z^{(i)}W_K, Z^{(i)}W_Q, Z^{(i)}W_V) $$
    $$ = \text{softmax}\left( \frac{Z^{(i)}W_K W_V^T Z^{(i)T}}{d^{1/2}} \right) Z^{(i)}W_V $$
2.  **残差与归一化**:
    $$ \tilde{Z} := \text{LayerNorm}(Z^{(i)} + \tilde{Z}) $$
3.  **前馈网络 (FFN) 与残差**:
    $$ Z^{(i+1)} := \text{LayerNorm}(\text{ReLU}(\tilde{Z}W_1)W_2 + \tilde{Z}) $$

### 结构解读
虽然看起来复杂，但本质上就是：
**Self-attention** $\to$ **Linear layer + ReLU**
中间穿插了有些“随意”放置的**残差连接 (Residual connections)** 和 **归一化 (Normalization)**（具体放置位置在不同变体中经常调整，如 Pre-Norm vs Post-Norm）。

![[Placeholder: Slide 14 Transformer block internal diagram]]

---

## 12. Transformer 应用于时间序列的优缺
*(对应第 15 页)*

我们可以将 Transformer 块应用于时间序列的“直接”预测方法，替代卷积块。

### 优点 (Pros)
*   **全感受野 (Full receptive field)**: 在单层内即可覆盖全部历史信息（即可以立即使用过去所有的数据）。
*   **参数效率**: 随时间混合信息不会增加参数数量（不像卷积，感受野越大通常核越大或层越深）。

### 缺点 (Cons)
*   **非因果性 (Acausal)**: 所有的输出依赖于所有的输入（默认情况下，这对于自回归/预测任务是不好的，因为能看到未来）。
*   **无序性 (No ordering)**: 数据没有内在的顺序概念（因为 Transformer 对序列的排列是等变的）。

---

## 13. Masked Self-Attention (掩码自注意力)

### 解决非因果性问题
为了解决依赖“未来”信息的问题，我们可以**掩码 (mask)** softmax 操作符，将任何“未来”时间步的权重设为零。

$$ \text{softmax}\left( \frac{KQ^T}{d^{1/2}} - M \right) V $$

其中，$M$ 是一个矩阵：
*   上三角部分（未来）为 $\infty$（softmax后变为0）。
*   下三角部分（过去及现在）为 $0$。

$$ M = \begin{bmatrix} 0 & \infty \\ 0 & 0 \end{bmatrix} $$

> [!tip] 实现细节
> 即使技术上我们可以“避免”创建这些不需要的注意力矩阵条目，但在实践中，先生成完整的矩阵然后将其掩盖（mask out）通常更快（利用矩阵运算的并行性）。

---

## 14. Positional Encodings (位置编码)
*(对应第 17 页)*

### 解决顺序无关性问题
为了解决“顺序不变性/等变性”问题，我们可以向输入添加**位置编码 (positional encoding)**，将每个输入与其在序列中的位置关联起来。

$$ X \in \mathbb{R}^n = \begin{bmatrix} - x_1^T - \\ - x_2^T - \\ \vdots \\ - x_T^T - \end{bmatrix} + \begin{bmatrix} \sin(\omega_1 \cdot 1) & \cdots & \sin(\omega_n \cdot 1) \\ \sin(\omega_1 \cdot 2) & \cdots & \sin(\omega_n \cdot 2) \\ \vdots & \ddots & \vdots \\ \sin(\omega_1 \cdot T) & \cdots & \sin(\omega_n \cdot T) \end{bmatrix} $$

*   $\omega_i$ ($i=1 \dots n$) 通常按照对数刻度（logarithmic schedule）选择频率。
*   实际上，是将位置编码加到 $X$ 的 $d$ 维投影上。
*   这样模型就能通过编码的值区分 $x_1$ 是在 $x_2$ 之前还是之后。

---

## 15. Transformers 超越时间序列 (Beyond Time Series)
*(对应第 19 页)*

最近的研究发现，Transformer 块在时间序列之外的领域也非常强大。

### 主要应用
*   **Vision Transformers (ViT)**:
    *   将 Transformer 应用于图像。
    *   图像被表示为一系列的**补丁嵌入 (patch embeddings)**。
    *   在大数据集上效果优于 CNN。
*   **Graph Transformers**:
    *   在注意力矩阵中捕捉图结构（Graph structure）。

### 存在的挑战
在所有这些应用中，面临的挑战包括：
1.  **数据表示**: 如何表示数据以使 $O(T^2)$ 的操作是可行的（序列长度 $T$ 不能太长）。
2.  **位置嵌入**: 如何形成 2D 或图结构的位置嵌入（Positional embeddings）。
3.  **掩码矩阵**: 如何形成掩码矩阵（Mask matrix）以适应特定的依赖关系。