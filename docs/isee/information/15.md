# 第五讲 信息处理：计算理论

## 第一部分：计算理论

1. **计算的最基本概念：对于一个给定输入，获得期望输出的过程。**
2. **算法 (Algorithm)**：就是计算的方法。它是接收输入并产生输出的计算过程。

> [!info] 算法的四个核心性质
> *   **能行性 (Effectiveness)**：算法的每一步都必须是清晰、可执行的，有明确的求解步骤。
> *   **有穷性 (Finiteness)**：算法在执行有限的步骤后必须终止，并给出结果。
> *   **通用性 (Generality)**：算法应能解决一类问题，而不是一个特定的问题实例，并总是给出正确答案。
> *   **确定性 (Definiteness)**：算法的每一步都是机械的、明确的，没有歧义。

### 计算模型：图灵机

为了严格地研究计算，我们需要一个数学上精确的计算模型。**图灵机 (Turing Machine)** 就是这样一个模型。

#### 图灵机的定义

*   **计算模型**：刻画计算的抽象形式系统或数学系统。
*   **图灵机**：
    *   1936年由艾伦·图灵提出，是现代计算机的**理论模型**。
    *   **与现代计算机的相同之处**：程序与数据混合在一起，由控制器控制执行。
    *   **与现代计算机的不同之处**：
        1.  **内存无限大**：它有一条可以无限延伸的带子。
        2.  **没有独立的输入/输出**：所有信息（输入、中间结果、输出）都在这条带子上。

#### 图灵机的直观描述

![|550](Pasted%20image%2020251210101637.png)

一个图灵机由三个核心部分组成：

1.  **一条无限长的带子 (Tape)**：
    *   带子被分成一个个的小方格。
    *   每个方格可以写入一个符号。
    *   **有穷字母表**：所有可以写入的符号的集合，例如 `{S₀, S₁, S₂, ...}`。通常我们简化为二进制 `{0, 1}` 和一个空格符号 `b`。

2.  **一个读写头 (Head)**：
    *   可以在带子上**左右移动**。
    *   可以**读取**当前方格的符号。
    *   可以**改写**当前方格的符号。

3.  **一个有限状态控制器 (Controller)**：
    *   机器在任何时刻都处于一个特定的**状态**中，状态的集合是有限的，例如 `{q₁, q₂, ..., qₙ}`。
    *   控制器内部包含一套**指令集**，告诉机器在**当前状态**下，**读到某个符号**时，应该**做什么**。

> [!abstract] 图灵机的指令：五元组
> 图灵机的行为由一系列五元组指令决定，格式如下：
> **$(q_i, S_j, S_k, D, q_{next})$**
>
> *   $q_i$：机器**当前**所处的状态。
> *   $S_j$：读写头从当前方格**读入**的符号。
> *   $S_k$：要**写入**当前方格的新符号（可以和 $S_j$ 相同）。
> *   $D$：读写头的**移动方向**（R: 右移一格, L: 左移一格, N: 不移动）。
> *   $q_{next}$：机器将要转移到的**下一个**状态。
>
> **简单来说，指令就是：“如果我在状态 $q_i$，读到了符号 $S_j$，那么我就把这个符号改成 $S_k$，然后把读写头向 $D$ 方向移动一格，最后将我的状态更新为 $q_{next}$。”**

#### 图灵机的工作原理与设计问题 (Slide 14)

*   **工作原理**：
    1.  机器从一个**初始状态** ($q_1$) 和带子上的某个起始位置出发。
    2.  根据当前状态和读到的符号，查找匹配的五元组指令并执行。
    3.  不断重复第2步，直到进入一个**结束状态** ($q_n$) 或者没有指令可以执行为止。
    4.  机器停止时，带子上的内容就是**计算结果**。

*   **指令设计问题**：设计指令集时必须避免：
    *   **指令死循环**：例如 `q₁ S₂S₂Rq₃; q₃ S₃S₃Lq₁`，机器可能在状态 `q₁` 和 `q₃` 之间来回振荡，永远无法停止。
    *   **指令二义性**：例如 `q₃ S₂S₂Rq₄; q₃ S₂S₄Lq₆`，当机器处于状态 `q₃` 读到 `S₂` 时，有两条指令都可以执行，这在确定型图灵机中是不允许的。

---

> [!example] 例题1：图灵机实现加一运算 (Slide 15-16)
> **问题描述**:
> 假设 `b` 表示空格，`q₁` 是初始状态，`q₄` 是结束状态。带子上的输入信息为二进制数 `10100010`。读写头初始位置对准最右边的第一个 `0`，机器处于状态 `q₁`。
>
> **目标**：执行下面的指令集，看最终输出是什么。
>
> **指令集**:
> 1.  `q₁01Lq₂`
> 2.  `q₁10Lq₃`
> 3.  `q₁bbNq₄` (b是空格)
> 4.  `q₂00Lq₂`
> 5.  `q₂11Lq₂`
> 6.  `q₂bbNq₄`
> 7.  `q₃01Lq₂`
> 8.  `q₃10Lq₃`
> 9.  `q₃bbNq₄`
>
> **执行过程追踪**:
> 我们把带子状态表示为 `...左边 [读写头位置] 右边...`，状态为 `q`。
>
> 1.  **初始状态**: `...b b 1 0 1 0 0 0 1 [0] b b...`，状态 `q₁`
> 2.  **读到 `0`，状态 `q₁`**: 匹配指令 `q₁01Lq₂`。
>     *   动作：将 `0` 改为 `1`，读写头左移 (L)，状态变为 `q₂`。
>     *   **新状态**: `...b b 1 0 1 0 0 0 [1] 1 b b...`，状态 `q₂`
> 3.  **读到 `1`，状态 `q₂`**: 匹配指令 `q₂11Lq₂`。
>     *   动作：将 `1` 改为 `1`，读写头左移 (L)，状态保持 `q₂`。
>     *   **新状态**: `...b b 1 0 1 0 0 [0] 1 1 b b...`，状态 `q₂`
> 4.  **读到 `0`，状态 `q₂`**: 匹配指令 `q₂00Lq₂`。
>     *   动作：将 `0` 改为 `0`，读写头左移 (L)，状态保持 `q₂`。
>     *   **新状态**: `...b b 1 0 1 0 [0] 0 1 1 b b...`，状态 `q₂`
> 5.  ... 读写头会一直向左移动，遇到 `0` 或 `1` 都保持不变，直到遇到空格 `b`。
>     *   **... 状态**: `...b [b] 1 0 1 0 0 0 1 1 b b...`，状态 `q₂`
> 6.  **读到 `b`，状态 `q₂`**: 匹配指令 `q₂bbNq₄`。
>     *   动作：将 `b` 改为 `b`，读写头不移动 (N)，状态变为 `q₄`。
>     *   **结束状态**: `...b [b] 1 0 1 0 0 0 1 1 b b...`，状态 `q₄`
>
> **分析与结论**:
> 输入是 `10100010`。经过图灵机运算后，带子上的数变成了 `10100011`。
>
> 这个图灵机实现的功能是**二进制加一**，即计算函数 $S(x) = x + 1$。
>
> *   指令 `q₁0...` 和 `q₁1...` 处理的是二进制数的最低位，实现加法和进位逻辑。
> *   状态 `q₂` 代表“加法已完成，无需进位”，只需找到数字的开头。
> *   状态 `q₃` 代表“需要继续进位”，`0` 变成 `1` 后转到 `q₂` (停止进位)，`1` 变成 `0` 后继续 `q₃` (继续进位)。
>
> 请在此处插入第16页的图片，它直观地展示了这个过程。

> [!example] 例题2：来自2020-2021年考卷 (改编)
> **问题描述**:
> 有一个图灵机，初始状态为 `q₁`，结束状态为 `q₄`，输入为 `11001100`，读写头对准最右边的 `0`。运行以下指令集，输出结果是什么？
>
> **指令集**:
> 1. `q₁01Lq₂`
> 2. `q₁10Lq₃`
> 3. `q₁bbNq₄`
> 4. `q₂01Lq₁` (*注意：这里与上例不同*)
> 5. `q₂10Lq₃` (*注意：这里与上例不同*)
> 6. `q₂bbNq₄`
> 7. `q₃01Lq₂`
> 8. `q₃10Lq₃`
> 9. `q₃bbNq₄`
>
> **分析**:
> 这个指令集与上一个例子非常相似，但 `q₂` 状态的行为有所不同。我们同样可以追踪这个过程。可以发现，这个图灵机也在执行某种算术运算。这是一个很好的练习，可以帮助你理解状态转移和指令如何共同决定计算结果。

---
### 4. 现代计算机：冯·诺依曼结构

图灵机是理论模型，而我们日常使用的计算机大多基于**冯·诺依曼结构**。

#### 核心思想 (Slide 18-19)

请在此处插入第18页的图片，展示冯诺依曼结构图。

*   **五大组成部分**：
    1.  **运算器 (ALU)**：执行算术和逻辑运算。
    2.  **控制器 (Control Unit)**：指挥计算机各部分工作。
    3.  **存储器 (Memory)**：存放数据和指令。
    4.  **输入设备 (Input)**：如键盘、鼠标。
    5.  **输出设备 (Output)**：如显示器、打印机。
    *   (运算器和控制器合称为中央处理器 **CPU**)

*   **基本原理：存储程序 (Stored Program)**
    *   这是冯·诺依曼结构最核心的思想：**指令和数据以同等地位存储在存储器中**，并可按地址寻访。
    *   计算机启动后，控制器会按照程序指定的顺序，逐条从存储器中取出指令和数据，并加以执行，自动完成任务。

#### 特点与局限性 (Slide 20)

*   **特点**:
    *   以运算器为中心（早期设计）。
    *   采用**存储程序**原理。
    *   指令由操作码和地址码组成。
    *   数据以二进制表示和运算。
    *   硬件和软件完全分离。

*   **局限性：冯·诺依曼瓶颈 (Von Neumann bottleneck)**
    *   CPU 和存储器之间的通路（总线）是数据流的唯一通道。
    *   执行指令时，需要先从内存取指令，再从内存取数据，这导致CPU和内存之间的通信非常频繁，总线带宽成为了性能瓶颈。

### 5. 可计算性理论

这一部分探讨哪些问题是计算机**能够**解决的。

#### 希尔伯特第10问题 (Slide 22)

> [!challenge] **判定丢番图方程的可解性**
>
> **问题**: 给定一个系数为整数的丢番图方程（多变量整系数多项式方程），是否存在一个通用的算法，能在有限步骤内判定该方程是否有整数解？
>
> **答案**: **不存在**。
>
> **马蒂雅谢维奇定理** (1970年) 证明了这个问题是**不可解**的。这意味着我们永远无法编写一个完美的程序，对**任意**一个丢番图方程，都能正确地回答“有解”或“无解”。

#### 丘奇-图灵论题 (Church-Turing Thesis) (Slide 22)

这是一个非常深刻的论题，它连接了直觉上的“可计算”和数学上的“图灵机可计算”。

> [!quote] 丘奇-图灵论题
> **任何在算法上（直觉上）可有效计算的问题，同样可由图灵机有效计算。**
>
> 换句话说，任何我们可以通过纸笔、遵循一系列明确规则来解决的问题（忽略时间和资源限制），图灵机也都能解决。这个论题断言，**图灵机已经捕捉到了“可计算”这一概念的本质**。至今没有发现反例。

#### 停机问题 (The Halting Problem) (Slide 23-27)

这是可计算性理论中最著名、最重要的**不可判定问题**。

> [!important] 停机问题的定义
> **通俗地说，停机问题就是：是否存在一个通用的程序 H，对于任意给定的程序 P 和输入 I，H 能够判断出“程序 P 在输入 I 的情况下最终是否会停止运行”。**
>
> 答案是：**不存在这样的通用程序 H**。停机问题是**不可解**的。

*   **简单的例子 (Slide 24)**：
    *   `while (i < 10) { i++; }`: 这个循环会停机。
    *   `while (i > 0) { i++; }`: 这个循环永远不会停机（死循环）。
    对于简单的程序，我们人脑可以判断。但对于极其复杂的程序，判断它是否会停机是非常困难的，甚至是不可能的。

*   **证明思路：反证法 (Proof by Contradiction)** (Slide 25-26)

    请在此处插入第26页的图片，它清晰地展示了证明逻辑。

    1.  **假设存在**：我们假设存在这样一个万能的测试程序 `T`。
        *   `T(P)` -> `1` (表示P会终止)
        *   `T(P)` -> `0` (表示P不会终止)

    2.  **构造一个“悖论”程序 S**:
        我们基于 `T` 构造一个新程序 `S`，它的逻辑如下：
        *   `S` 接受一个程序 `P` 作为输入。
        *   `S` 内部调用 `T` 来测试 `P`，即计算 `T(P)`。
        *   如果 `T(P)` 的结果是 `1` (意味着 `P` 会终止)，那么 `S` 就故意进入一个**死循环**。
        *   如果 `T(P)` 的结果是 `0` (意味着 `P` 不会终止)，那么 `S` 就立刻**终止**。

    3.  **把 S 作为自己的输入**: 现在，我们来运行 `S(S)`，问：`S(S)` 究竟是终止还是不终止？
        *   **情况A：如果 `S(S)` 终止了**
            *   根据 `S` 的定义，`S` 会终止，当且仅当它内部的测试 `T(S)` 返回 `0`。
            *   `T(S)` 返回 `0` 意味着程序 `S` **不会终止**。
            *   **矛盾**！我们假设 `S(S)` 终止，却推导出 `S` 不会终止。

        *   **情况B：如果 `S(S)` 不终止（死循环）**
            *   根据 `S` 的定义，`S` 会进入死循环，当且仅当它内部的测试 `T(S)` 返回 `1`。
            *   `T(S)` 返回 `1` 意味着程序 `S` **会终止**。
            *   **矛盾**！我们假设 `S(S)` 不终止，却推导出 `S` 会终止。

    4.  **结论**: 无论哪种情况都会导致逻辑矛盾。唯一的解释是，我们最初的假设是错误的。因此，**万能的测试程序 `T` 根本不存在**。停机问题是不可解的。

*   **理发师悖论 (Barber's Paradox) (Slide 27)**：这是一个帮助理解停机问题背后逻辑矛盾的经典例子。
    *   一个理发师声称：“我只给城里所有不自己刮脸的男人刮脸。”
    *   问题：谁给这位理发师刮脸？
        *   如果他自己刮：他属于“自己刮脸”的人，但他的招牌说他不给这类人刮脸。矛盾。
        *   如果他不自己刮：他属于“不自己刮脸”的人，根据招牌，他应该给自己刮脸。矛盾。

### 6. 计算复杂性理论

这一部分研究那些**可解**问题中，哪些是**容易**的，哪些是**困难**的。

*   **例子 (Slide 29)**
    *   **排序问题**：给100万个数排序。这是一个**容易**的问题，有很多高效的算法（如快速排序），计算机可以在短时间内完成。
    *   **时间表问题**：为大学排课程表，满足各种限制（如教室不能冲突，老师时间不能冲突等）。这是一个**困难**的问题。当课程和限制增多时，找到一个完美解所需的时间会爆炸式增长。

> [!help] 复杂性理论的核心问题
> **是什么使得某些问题很难计算，而又使另一些问题容易计算呢？**

#### 复杂性度量

*   **计算复杂性**：用计算机求解问题的难易程度。它针对的是**问题本身**的固有难度。
    *   `计算复杂性 = 解决该问题的最佳算法的复杂性`
*   **度量标准**：
    *   **时间复杂度**：计算所需的步数，通常用输入规模 `n` 的函数来表示。
    *   **空间复杂度**：计算所需的存储空间大小。
*   **大O记法 (Big O Notation)**：我们通常关心复杂度的**渐进趋势**。
    *   $f(n) = 6n^3 + 4n^2 + 7 = O(n^3)$ (多项式界，认为是**容易**的)
    *   $f(n) = O(2^n)$ (指数界，认为是**困难**的)

#### P类问题

> [!definition] **P (Polynomial Time)**
> **P类问题**是所有可以由一个**确定型图灵机**在**多项式时间**内**解决**的判定问题。
>
> 这类问题被认为是“容易计算”或“高效可解”的。
>
> **例子**: 有向图路径问题 (PATH) - 判断一个图中从点s到点t是否存在路径。这个问题有 $O(n)$ 
的高效算法。
>
> ![|450](Pasted%20image%2020251210101405.png)

#### NP类问题 (Slide 35)

> [!definition] **NP (Nondeterministic Polynomial Time)**
> **NP类问题**是所有可以由一个**非确定型图灵机**在**多项式时间**内**解决**的判定问题。
>
> 一个更直观的定义是：
> **如果一个问题的某个解可以在多项式时间内被验证其正确性，那么这个问题就是NP问题。**
>
> 这类问题不一定容易“解决”，但很容易“验证”。
>
> **例子**: 哈密顿路径问题 (HAMPATH) - 判断一个图中是否存在一条经过每个顶点恰好一次的路径。
> *   **解决**：找到这样一条路径非常困难。
> *   **验证**：如果有人给了你一条路径，你只需要检查它是否访问了所有顶点且每个只访问一次，这个验证过程非常快（多项式时间）。

#### P vs NP 问题 (Slide 36)

请在此处插入第36页的图片。

*   我们知道，所有P类问题都属于NP类问题（`P ⊆ NP`）。因为如果一个问题能被快速解决，那么它的解自然也能被快速验证。
*   **世纪难题**: **P = NP ?**
    *   这询问的是：**每一个解能够被快速验证的问题，是否也能够被快速地解决？**
    *   绝大多数科学家相信 `P ≠ NP`，但至今无人能证明。
    *   这是克雷数学研究所提出的七个“百万美元大奖问题”之一。

#### 复杂性理论的应用：密码学 (Slide 37)

*   **RSA算法**：是目前应用最广泛的公钥加密算法。
*   其安全性依赖于一个数学难题：**大整数因子分解**。
    *   将两个大素数相乘很容易（P问题）。
    *   但将一个巨大的合数分解成两个素数的乘积则非常困难（被认为是NP问题，但不是P问题）。
    *   如果有一天有人证明了 `P = NP` 并找到了一个高效的因子分解算法，那么整个现代加密体系将瞬间崩溃。

### 7. Kolmogorov 复杂度

这是一种衡量**单个对象**（如一个字符串）复杂性的方法，也称为**算法信息论**。

#### 核心思想 (Slide 38)

一个字符串的复杂度，可以用**能够生成这个字符串的最短程序的长度**来衡量。

*   `序列1: 010101010101010101010101010101010101010101010101010101010101`
    *   描述很简单：“打印‘01’ 30次”。这个描述（程序）很短。
    *   **复杂度低**。
*   `序列3: 1101111001110101111011011110101101101110001011001010010011011`
    *   看起来毫无规律，像随机噪声。
    *   最短的描述可能就是“打印‘110111...’”。这个描述和字符串本身一样长。
    *   **复杂度高**。

#### 定义与性质 (Slide 39-42)

> [!abstract] Kolmogorov复杂度的定义
> 对于一个通用计算机 $U$，字符串 $x$ 的Kolmogorov复杂度 $K_U(x)$ 定义为：
> $$ K_U(x) = \min_{p: U(p)=x} l(p) $$
> 其中，$p$ 是一个程序，$U(p)=x$ 意味着程序 $p$ 在计算机 $U$ 上运行会输出字符串 $x$ 并停止，$l(p)$ 是程序 $p$ 的长度。
>
> 简单说，$K(x)$ 就是**能生成 $x$ 的最短程序的长度**。它反映了字符串 $x$ 的**信息内容、冗余度、结构化程度**。

*   **通用性 (不变性定理)**: 对于任何两台通用计算机 $U$ 和 $A$，它们计算出的复杂度最多只相差一个常数：$K_U(x) \le K_A(x) + c_A$。这意味着复杂度的定义是稳健的，不依赖于特定的计算机或编程语言。
*   **上界**: $K(x) \le l(x) + c$。一个字符串的复杂度不会比它自身的长度大太多，因为总可以用 "print 'x'" 这样的程序来生成它。
*   **下界**: 大部分字符串都是不可压缩的。复杂度小于 $k$ 的字符串数量少于 $2^k$ 个。

#### 与熵的关系 (Slide 45)

Kolmogorov复杂性与信息论中的**熵**有深刻的联系。对于一个由独立同分布的随机过程产生的长序列 $X^n$，我们有：
$$ \lim_{n \to \infty} E\left[\frac{1}{n} K(X^n|n)\right] = H(X) $$
这说明，一个随机信源的**平均每个符号的香农熵**，等于该信源产生的**长序列的平均每符号的算法复杂度**。熵衡量了整体的平均不确定性，而Kolmogorov复杂度衡量了单个实例的随机性。

---
## 第二部分：知识转换原理与机器学习

这一部分从更宏观的角度探讨信息如何转化为知识，并引入了实现这一过程的强大工具——机器学习。

### 1. 基本概念与知识生成

#### 信息、经验、知识、策略 (Slide 50)

这是一个递进的层次关系：
*   **信息**：对事物运动状态的描述。
*   **经验**：对信息（事物运动状态）的重复体验。
*   **知识**：从经验中总结出的**规律**。
*   **策略**：基于知识形成的解决问题的**方法**。

#### 数据预处理 (Slide 52-53)

现实世界的数据（信息）往往是“脏”的，需要处理才能使用。
*   **常见问题**：不一致、重复、不完整（缺失值）、含噪声、维度高。
*   **处理方法**：
    1.  **数据清洗**：填充缺失值，平滑噪声，解决不一致。
    2.  **数据集成**：合并多个数据源。
    3.  **数据变换**：规范化数据（如缩放到区间）。
    4.  **数据归约**：降低数据维度，压缩数据。

#### 知识的生成方法 (Slide 55-58)

1.  **归纳推理 (Induction)**：**从个别到一般**。
    *   从大量的具体事例中总结出一般性规律。
    *   例如，看到“麻雀会飞”、“鸽子会飞”、“燕子会飞”，归纳出“鸟会飞”。
    *   归纳的结论**不保证绝对正确**（比如鸵鸟就不会飞）。

2.  **演绎推理 (Deduction)**：**从一般到个别**。
    *   利用已有的知识（大前提），通过逻辑推理生成新的知识。
    *   例如：大前提“人都是要死的”，小前提“苏格拉底是人”，结论“苏格拉底是要死的”。
    *   演绎的结论具有**必然性**。

### 2. 机器学习入门

机器学习是让计算机自动从数据中学习规律（知识）的学科。

> [!definition] 机器学习的定义 (T. M. Mitchell)
> 研究如何“**利用经验来改善计算机系统自身的性能**”的学科。
>
> *   这里的“经验”就是**数据**。
> *   “性能”是通过某个**任务**来衡量的。
> *   “改善”是指在任务上的表现随着经验的增加而变好。

#### 机器学习 vs 人类学习 (Slide 60)

请在此处插入第60页的图片。

*   **人类学习**：通过**经验** -> **归纳**出**规律** -> 用于**预测**未来。
*   **机器学习**：使用**历史数据** -> **训练**出**模型** -> 用于**预测**未知属性。

#### 机器学习方法分类 (Slide 62)

1.  **监督学习 (Supervised Learning)**：
    *   **数据**：带有**标签**（正确答案）的训练数据。
    *   **任务**：学习一个从输入到输出的映射函数。
    *   **例子**：**分类**（如判断邮件是否为垃圾邮件）、**回归**（如预测房价）。

2.  **非监督学习 (Unsupervised Learning)**：
    *   **数据**：没有标签的数据。
    *   **任务**：从数据中发现隐藏的结构或模式。
    *   **例子**：**聚类**（如将客户分群）、**密度估计**。

3.  **强化学习 (Reinforcement Learning)**：
    *   **模式**：智能体(Agent)在环境中通过**试错**来学习。
    *   **机制**：通过环境给予的**奖励**或**惩罚**来调整行为，目标是最大化长期累积奖励。
    *   **例子**：训练AI下棋、玩游戏、机器人控制。

#### 机器学习基本过程 (Slide 63)

1.  **表示 (Representation)**：将数据对象进行**特征化**，转换成计算机可以处理的向量。
2.  **训练 (Training/Learning)**：给定一个数据集，从中学习出规律（**模型**）。目标是模型不仅在训练数据上表现好，在未知数据上也要表现好（**泛化能力**）。
3.  **测试 (Testing/Inference)**：对于一个新的数据样本，利用学到的模型进行**预测**。

#### 关键问题：过拟合 (Over-fitting) (Slide 66-68)

*   **现象**：模型在**训练数据**上表现得过于完美，以至于把数据中的噪声和偶然性也学进去了，导致在**测试数据**（未知数据）上表现很差。
*   **原因**：模型过于复杂（如多项式次数太高）。
*   **解决方法**：
    *   **增加训练样本数**：更多的数据可以帮助模型学习到更本质的规律。
    *   **规范化 (Regularization)**：在误差函数中加入一个惩罚项，限制模型的复杂度。
        *   原始误差函数：$\tilde{E}(\mathbf{w}) = \frac{1}{2}\sum_{n=1}^{N}\{y(x_n, \mathbf{w}) - t_n\}^2$
        *   加入正则项后：$\tilde{E}(\mathbf{w}) = \frac{1}{2}\sum_{n=1}^{N}\{y(x_n, \mathbf{w}) - t_n\}^2 + \frac{\lambda}{2}||\mathbf{w}||^2$ (L2正则化)
        *   $||\mathbf{w}||^2$惩罚大的权重，使得模型更平滑，防止剧烈波动。$\lambda$控制惩罚的强度。

### 3. 机器学习方法详解

#### 贝叶斯推理 (Bayesian Inference) (Slide 70-84)

这是一种基于概率论进行推理和学习的方法。

*   **核心思想**：根据已观察到的数据，来更新我们对一个假设的信任程度。
*   **贝叶斯公式 (Bayes' Theorem) (Slide 72)**：
    $$ P(h|D) = \frac{P(D|h)P(h)}{P(D)} $$
    *   $P(h)$: **先验概率 (Prior)**。在看到任何数据之前，我们对假设 $h$ 的信任程度。
    *   $P(D|h)$: **似然度 (Likelihood)**。在假设 $h$ 成立的情况下，观察到数据 $D$ 的概率。
    *   $P(h|D)$: **后验概率 (Posterior)**。在观察到数据 $D$ 之后，我们对假设 $h$ 的新的信任程度。
    *   $P(D)$: 证据因子，用于归一化。

*   **决策规则 (Slide 73)**
    1.  **最大后验 (MAP) 决策**: 寻找后验概率 $P(h|D)$ 最大的假设 $h$。
        $h_{MAP} = \arg\max_{h \in H} P(h|D) = \arg\max_{h \in H} P(D|h)P(h)$
    2.  **最大似然 (ML) 决策**: 如果我们对所有假设的先验概率一无所知，就假设它们是均等的 ($P(h_i) = P(h_j)$)。此时，MAP决策退化为最大似然决策，即寻找似然度 $P(D|h)$ 最大的假设 $h$。
        $h_{ML} = \arg\max_{h \in H} P(D|h)$

> [!example] 例题：医疗诊断 (Slide 74-75)
> **问题**: 一个人检测结果为阳性[+]，应该诊断他有病(ill)还是没病(¬ill)？
>
> **已知信息**:
> *   先验概率: $P(\text{ill}) = 0.008$, $P(\neg\text{ill}) = 0.992$ (这种病很罕见)
> *   似然度 (化验准确率):
>     *   $P(+|\text{ill}) = 0.98$ (真阳性率)
>     *   $P(-|\text{ill}) = 0.02$ (假阴性率)
>     *   $P(+|\neg\text{ill}) = 0.03$ (假阳性率)
>     *   $P(-|\neg\text{ill}) = 0.97$ (真阴性率)
>
> **决策分析**:
> 1.  **ML (最大似然) 决策**: 我们只看“阳性”这个结果更容易由哪个假设产生。
>     *   $P(+|\text{ill}) = 0.98$
>     *   $P(+|\neg\text{ill}) = 0.03$
>     *   因为 $0.98 > 0.03$，ML决策会认为这个人 **有病** ($h_{ML} = \text{ill}$)。
>
> 2.  **MAP (最大后验) 决策**: 我们需要结合先验概率来计算后验概率。
>     *   $P(\text{ill}|+) \propto P(+|\text{ill}) P(\text{ill}) = 0.98 \times 0.008 = 0.0078$
>     *   $P(\neg\text{ill}|+) \propto P(+|\neg\text{ill}) P(\neg\text{ill}) = 0.03 \times 0.992 = 0.0298$
>     *   因为 $0.0298 > 0.0078$，MAP决策会认为这个人 **没病** ($h_{MAP} = \neg\text{ill}$)。
>
> **结论**:
> *   ML决策忽略了“这种病本身很罕见”这个重要的先验知识，得出了一个可能错误的结论。
> *   MAP决策结合了数据证据和先验知识，给出了一个更合理的判断。即使检测为阳性，更可能的情况是“一个健康人被误报了”（假阳性），而不是“一个病人被正确检出了”。
> *   (计算完整的后验概率：$P(D) = P(+) = 0.0078 + 0.0298 = 0.0376$。所以 $P(\text{ill}|+) = 0.0078/0.0376 \approx 0.21$, $P(\neg\text{ill}|+) = 0.0298/0.0376 \approx 0.79$)

*   **朴素贝叶斯分类器 (Naive Bayes Classifier) (Slide 80-84)**
    *   一种简单但非常有效的分类算法。
    *   **“朴素”的假设**: **特征之间条件独立**。即假定在给定目标值的情况下，一个特征的属性值与其他特征的属性值无关。
    *   **决策规则**:
        $v_{NB} = \arg\max_{v_j \in V} P(v_j) \prod_i P(a_i|v_j)$
        *   $v_j$: 某个类别（如 "Yes", "No"）。
        *   $a_i$: 第 $i$ 个特征的属性值（如 Outlook="Sunny"）。
        *   $P(v_j)$: 类别的先验概率。
        *   $P(a_i|v_j)$: 在类别 $v_j$ 下，出现属性 $a_i$ 的概率。

> [!example] 例题：Play Tennis (Slide 82-84)
> **问题**: 根据14天的历史数据，判断在 `Outlook=Sunny, Temp=Hot, Humidity=High, Wind=Strong` 这一天是否适合打网球 (Yes/No)？
> (注意：PPT中计算的是 Cool, 这里我们按题目要求用 Hot)
>
> **计算步骤**:
> 1.  **计算先验概率**:
>     *   $P(\text{Yes}) = 9/14 \approx 0.64$
>     *   $P(\text{No}) = 5/14 \approx 0.36$
> 2.  **计算各特征的条件概率**: (从Slide 83的表格中数数)
>     *   $P(\text{Sunny}|\text{Yes}) = 2/9$
>     *   $P(\text{Hot}|\text{Yes}) = 2/9$
>     *   $P(\text{High}|\text{Yes}) = 3/9$
>     *   $P(\text{Strong}|\text{Yes}) = 3/9$
>     ---
>     *   $P(\text{Sunny}|\text{No}) = 3/5$
>     *   $P(\text{Hot}|\text{No}) = 2/5$
>     *   $P(\text{High}|\text{No}) = 4/5$
>     *   $P(\text{Strong}|\text{No}) = 3/5$
> 3.  **应用朴素贝叶斯公式**:
>     *   **P(Yes|...):** $P(\text{Yes}) \times P(\text{Sunny}|\text{Yes}) \times P(\text{Hot}|\text{Yes}) \times P(\text{High}|\text{Yes}) \times P(\text{Strong}|\text{Yes})$
>         $= (9/14) \times (2/9) \times (2/9) \times (3/9) \times (3/9) \approx 0.0053$
>     *   **P(No|...):** $P(\text{No}) \times P(\text{Sunny}|\text{No}) \times P(\text{Hot}|\text{No}) \times P(\text{High}|\text{No}) \times P(\text{Strong}|\text{No})$
>         $= (5/14) \times (3/5) \times (2/5) \times (4/5) \times (3/5) \approx 0.0206$
> 4.  **结论**:
>     因为 $0.0206 > 0.0053$，所以朴素贝叶斯分类器预测结果为 **No**。

#### 决策树学习 (Decision Tree Learning) (Slide 85-91)

*   **决策树**: 一种树形结构，每个内部节点表示一个特征测试，每个分支代表一个测试输出，每个叶节点代表一个类别。
*   **核心问题**: 如何选择最佳的特征来分裂节点？
*   **标准**: **信息增益 (Information Gain)**。我们选择能使数据变得最“纯净”的特征。

> [!abstract] 熵和信息增益
> *   **熵 (Entropy)**：衡量一个数据集的**纯度**（不确定性）。熵越小，纯度越高。
>     $Entropy(S) = -p_+ \log_2 p_+ - p_- \log_2 p_-$
>     ($p_+, p_-$ 分别是集合S中正例和反例的比例)
> *   **信息增益 (Gain)**：表示在知道了特征 A 的值之后，数据不确定性减少的程度。增益越大，说明用特征 A 来划分数据效果越好。
>     $Gain(S, A) = Entropy(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} Entropy(S_v)$
>     ($S_v$ 是 S 中特征 A 的值为 v 的子集)
>
> **决策树构建算法 (ID3)**：
> 1. 计算所有特征的信息增益。
> 2. 选择信息增益最大的特征作为当前节点。
> 3. 根据该特征的不同取值建立分支。
> 4. 对每个分支的数据集，递归地重复1-3步。
> 5. 如果一个节点的数据都属于同一类别，或没有特征可用，则该节点成为叶节点。

> [!example] 例题：构造Play Tennis决策树 (Slide 89-91)
> **任务**: 为Play Tennis数据集选择第一个根节点。
>
> **计算过程**:
> 1.  **计算整个数据集的熵 (S=[9+, 5-])**:
>     $Entropy(S) = -(9/14)\log_2(9/14) - (5/14)\log_2(5/14) = 0.940$
> 2.  **计算各个特征的信息增益**:
>     *   **Gain(S, Outlook)**:
>         *   Values: Sunny, Overcast, Rain
>         *   $S_{Sunny}$=[2+,3-], $S_{Overcast}$=[4+,0-], $S_{Rain}$=[3+,2-]
>         *   $Gain = 0.940 - [\frac{5}{14}Entropy(S_{Sunny}) + \frac{4}{14}Entropy(S_{Overcast}) + \frac{5}{14}Entropy(S_{Rain})]$
>         *   $Gain = 0.940 - [\frac{5}{14}(0.971) + \frac{4}{14}(0) + \frac{5}{14}(0.971)] = 0.246$
>     *   **Gain(S, Humidity)**: (如图所示) = 0.151
>     *   **Gain(S, Wind)**: (如图所示) = 0.048
>     *   **Gain(S, Temperature)**: (计算后) = 0.029
> 3.  **结论**:
>     因为 **Outlook** 的信息增益最大 (0.246)，所以选择 **Outlook** 作为决策树的根节点。
>     接下来，对 Sunny, Overcast, Rain 三个分支的数据子集，重复这个过程，寻找下一个最佳分裂特征。

#### 基于实例的学习 (k-近邻) (Slide 92-98)

*   **核心思想 (懒惰学习)**：不建立显式的模型，只是把所有训练数据存储起来。当需要预测新实例时，查看它在特征空间中与哪些训练数据最“近”，然后根据这些“邻居”来做决定。
*   **k-近邻算法 (k-NN)**：
    1.  对于一个新的查询实例 $x_q$。
    2.  计算它与所有训练实例的距离（常用**欧氏距离**）。
    3.  找到距离最近的 $k$ 个训练实例（$k$ 个邻居）。
    4.  **分类问题**：在这 $k$ 个邻居中，采用“少数服从多数”的原则，哪个类别的邻居最多，就将 $x_q$ 判为哪个类别。
    5.  **回归问题**：取这 $k$ 个邻居的平均值作为 $x_q$ 的预测值。

*   **关键问题**:
    *   **维度魔咒**: 在高维空间中，所有点之间的距离都趋向于相等，"近邻"的概念变得模糊。
    *   **$k$ 值的选择**: $k$ 太小容易受噪声影响，$k$ 太大容易受不相关数据影响。
    *   **距离加权**: 可以给更近的邻居更大的权重。

#### 人工神经网络 (ANN) (Slide 99-110)

*   **思想**: 模仿人脑神经元的结构和功能。
*   **人工神经元模型 (感知器)** (Slide 101, 103):
    *   **输入**: 接收多个带权重的输入信号 $x_i$。
    *   **处理**: 将所有加权输入求和 $\sum w_i x_i$。
    *   **输出**: 将总和通过一个**激活函数 (Activation Function)** $f(\cdot)$ 转换后输出。$y = f(\sum w_i x_i)$。
*   **常见激活函数 (Slide 102)**:
    *   **符号函数**: 输出-1或+1，用于二分类。
    *   **Sigmoid函数**: $f(x) = \frac{1}{1+e^{-x}}$，输出(0,1)之间的平滑值，可用于表示概率。
*   **感知器的表征能力 (Slide 104)**:
    *   单个感知器只能表示**线性可分**的函数，它在n维空间中画出一条超平面来进行分割。
    *   可以实现 AND, OR, NOT 等。
    *   **不能**实现 **XOR (异或)**，因为异或问题是线性不可分的。
*   **多层感知器 (神经网络)** (Slide 109):
    *   通过将多个感知器组织成**层次结构**（输入层、隐藏层、输出层），可以表示任意复杂的非线性函数，从而解决XOR等问题。
*   **学习算法**:
    *   **感知器法则/Delta法则**: 用于单层感知器，通过梯度下降法调整权重 $w_i$ 以最小化输出误差。
    *   **反向传播算法 (Backpropagation)**: 用于多层网络，将输出层的误差逐层反向传播，来更新每一层的权重。

> [!example] 作业题：设计感知器 (来自Slide 118)
> **1. 设计一个两输入的感知器来实现布尔函数 `A ∧ ¬B` (A AND NOT B)**
>
> *   **目标**: 当 `A=1, B=0` 时输出1，其他情况输出0 (或-1)。
> *   **感知器模型**: $o = \text{sign}(w_1 A + w_2 B + w_0)$
> *   **设置权重**: 我们可以尝试设置权重。
>     *   需要 `A` 的权重为正，`B` 的权重为负。设 $w_1 = 1, w_2 = -1$。
>     *   $w_1 A + w_2 B = A - B$
>     *   当 `A=1, B=0` 时, $A-B = 1$。
>     *   当 `A=1, B=1` 时, $A-B = 0$。
>     *   当 `A=0, B=1` 时, $A-B = -1$。
>     *   当 `A=0, B=0` 时, $A-B = 0$。
> *   **调整偏置 $w_0$**: 我们需要一个阈值，使得只有当 $A-B > \text{threshold}$ 时才输出1。
>     *   设阈值为 0.5。即 $w_1 A + w_2 B + w_0 > 0$ 时输出1。
>     *   $A - B + w_0 > 0$。
>     *   $w_0$ 必须满足:
>         *   $1 - 0 + w_0 > 0 \implies w_0 > -1$
>         *   $1 - 1 + w_0 \le 0 \implies w_0 \le 0$
>         *   $0 - 1 + w_0 \le 0 \implies w_0 \le 1$
>         *   $0 - 0 + w_0 \le 0 \implies w_0 \le 0$
>     *   综合起来，我们可以取 $w_0 = -0.5$。
> *   **最终设计**: $w_1=1, w_2=-1, w_0=-0.5$。
>
> **2. 设计一个两层的感知器网络来实现异或 `A XOR B`**
>
> *   **提示**: `A XOR B = (A ∧ ¬B) ∨ (¬A ∧ B)`
> *   **设计思路**:
>     1.  设计一个隐藏层。隐藏层有两个神经元。
>     2.  第一个隐藏神经元实现 `A ∧ ¬B` (我们上面已经设计好了)。
>     3.  第二个隐藏神经元实现 `¬A ∧ B` (类似地可以设计出权重)。
>     4.  设计一个输出层神经元，对这两个隐藏神经元的输出执行 `OR` 操作。
> *   这是一个经典例子，说明多层网络如何通过组合简单的线性分类器来解决复杂的非线性问题。

#### 聚类 (Clustering) (Slide 111-117)

*   **任务 (非监督学习)**: 将一堆没有标签的数据自动分成若干个“簇”，使得**簇内**的数据相似度高，**簇间**的数据相似度低。
*   **k-均值算法 (k-means)** (Slide 112):
    1.  **初始化**: 随机选择 $k$ 个数据点作为初始的簇中心。
    2.  **分配**: 计算每个数据点到这 $k$ 个簇中心的距离，并将该点分配给距离最近的簇。
    3.  **更新**: 重新计算每个簇的中心（取簇内所有点的平均值）。
    4.  **迭代**: 重复步骤2和3，直到簇中心不再变化或变化很小为止。

*   **关键因素**:
    *   **初始中心点的选择**: 随机选择可能导致结果不稳定。
    *   **$k$ 值的选择**: 需要预先指定聚类的数量。
    *   **相异度的度量**: 如何定义“距离”或“相似度”。

---
希望这份详细的笔记能够帮助您更好地理解和掌握计算理论与机器学习的基础知识。祝您学习顺利！
