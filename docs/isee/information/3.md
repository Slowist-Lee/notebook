# 第三节、随机变量的平均互信息
## 一、随机变量间的平均互信息

### 1. 定义

引入：真币的一面是面值，一面是花纹。而假币的两面都是面值。现在随机抛某枚硬币，2次朝上的都是面值。求问，有多大概率知道它是真币？10次、100次呢？

此时，多次随机事件$x$的结果就会产生概率空间$\mathbf{X}$, 由是我们引入了对于概率空间$\{(X,Y),\mathcal{X}\times\mathcal{Y},p(x,y)\}$的定义和概念。

我们先简要回顾一下互信息的概念。我们当时说过，事件$X$和$Y$之间的互信息，即$Y$对$X$减少了多少的信息量，即：

两个事件$X=x$和$Y=y$之间相互提供的信息量定义为：

$I(x,y)=I(x)-I(x|y)=\log\frac{p(x|y)}{p(x)}=\log\frac{p(x,y)}{p(x)w(y)}$

类似地，我们给出对于$X$和$Y$,平均互信息的**定义**：

在概率空间$\{(X,Y),\mathcal{X}\times\mathcal{Y},p(x,y)\}$中，事件$(X,Y)$的平均互信息（简称互信息）为随机变量$I(x;y)$的数学期望：

$$I(X;Y)=E[I(x;y)]=\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log\frac{p(x,y)}{q(x)\omega(y)}$$
### 2. 性质

平均互信息有如下**性质**：

#### 2.1 非负性

！原本我们的互信息概念是有正、负、零三种可能的。

![](image/Pasted%20image%2020250329113611.png)

当且仅当$X$和$Y$独立时取等号。

（非负性可以用$H(X)\geq H(X|Y)$来理解）
#### 2.2 对称性

$I(X;Y)=I(Y;X)$

#### 2.3 互信息与熵的关联性

$I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)=H(X)+H(Y)-H(X,Y)$

![](image/Pasted%20image%2020250329113743.png)

物理意义：

互信息$I(X;Y)$等于$X$总的不确定性，减去$Y$已知以后，$X$中还留存的剩余不确定性。从而互信息代表了$Y$提供给$X$的信息量。

大小关系：

- $I(X;Y)\leq H(X)$,$X$是$Y$的确定性函数时，等号成立
- $I(X;Y)\leq H(Y)$,$Y$是$X$的确定性函数时，等号成立

### 3. 例题

例1：

![](image/Pasted%20image%2020250329114608.png)

解：

$I(X;Y)=H(X)-H(X|Y)$

计算一下概率值：

|            | X=0  | X=1  |$\sum P_x$|
| ---------- | ---- | ---- | ---------- |
| Y=0        | 0.49 | 0.02 | 0.51       |
| Y=1        | 0.49 | 0    | 0.49       |
|$\sum P_y$| 0.98 | 0.02 |            |

$\therefore H(X)=-0.98 \log_20.98-0.02\log_2 0.02=0.1414$

$H(X|Y)=E(X|y)=\sum_{y\in \mathcal{Y}} p(y)H(X|y)$

当$y=1$时，$p(0|1)=0.49/0.49=1$,$p(1|1)=0$
故$H(X|y)=H(1,0)=0$，不提供信息量

当$y=0$时，$p(0|0)=0.49/0.51=0.96$,$p(1|0)=0.02/0.51=0.04$
故$H(X|y)=H(0.96,0.04)$

$\therefore H(X|Y)=0.51\cdot H(0.96,0.04)=0.1236$

$I(X;Y)=H(X)-H(X|Y)=0.1414-0.1234=0.0178$

数据改变对信息量的影响：

![](image/Pasted%20image%2020250329120456.png)

例2:

![](image/Pasted%20image%2020250329120535.png)

解法1：

![](image/Pasted%20image%2020250329121252.png)

解法2：

![](image/Pasted%20image%2020250329121303.png)

## 二、条件互信息

我们之前定义的是事件的互信息，即：

$I(x;y|z)=\log \frac{p(x|y,z)}{p(x|z)}=\log \frac{p(x,y|z)}{p(x|z)p(y|z)}$

所以，在概率空间$\{(X,Y,Z),\mathcal{X}\times\mathcal{Y}\times\mathcal{Z},p(x,y,z)\}$中，事件$(X,Y)$在给定事件$Z$的条件下的条件互信息为：
$$I(X;Y|Z)=E[I(x;y|z)]=\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}\sum_{z\in\mathcal{Z}}p(x,y,z)\log\frac{p(x,y|z)}{q(x|z)\omega(y|z)}$$


## 三、联合互信息

我们之前定义事件的联合互信息，即：$I(x;(y,z))=I(x)-I(x|(y,z))$

在概率空间$\{(X,Y,Z),\mathcal{X}\times\mathcal{Y}\times\mathcal{Z},p(x,y,z)\}$中，事件$X$和事件$(Y,Z)$的联合互信息为：

$$I(X;(Y,Z))=E[I(x;(y,z))]=\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}\sum_{z\in\mathcal{Z}}p(x,y,z)\log\frac{p(x|y,z)}{q(x)}=I(X;Z)+I(X;Y|Z)$$

## 四、相对熵

### 1. 定义：

定义在相同字符表$\chi$上的两个概率分布$\{p(x)\}$和$\{q(x)\}$之间的**相对熵（散度）**， 或称为 <font color="#ff0000">K</font>ullback-<font color="#ff0000">L</font>eibler<font color="#ff0000">距离</font>，表示为：
$D(p//q)=\sum_x p(x)\log \frac{p(x)}{q(x)}=E_p\{\log \frac{p(x)}{q(x)}\}$

这个量能够表示实际分布$\{p(x)\}$与假定分布$\{q(x)\}$之间的平均差距，因而称为**鉴别熵**。

我们可以从图像上直观感受一下：

![](image/Pasted%20image%2020250329122045.png)

### 2. 性质

1.$D(p \parallel q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = - \sum_x p(x) \log \frac{q(x)}{p(x)}$
  $\geq - \sum_x p(x) \left( \frac{q(x)}{p(x)} - 1 \right) = 0$

2.$D(p \parallel q) \neq D(q \parallel p)$

下面我们要尝试用$D$来统一之前提到的$H$和$I$。

3.$I(X; Y) = \sum_{x,y} p(xy) \log \frac{p(xy)}{p(x)p(y)}$
  $= D(p(xy) \parallel p(x)p(y)) \geq 0$

4.设$U$是均匀分布，
$H(X)=-\sum_x p(x)\log p(x)=-\sum_xp(x)log\frac{p(x)}{1/K}+\log K=H(U)-D(X \parallel U)$

5.如果$P_1, P_2$是独立分布，并且联合分布是$P = P_1P_2$，如果$Q_1, Q_2$是独立分布，并且联合分布是$Q = Q_1Q_2$，那么 $D(P \parallel Q) = D(P_1 \parallel Q_1) + D(P_2 \parallel Q_2)$

### 3. 相对熵的应用

![](image/Pasted%20image%2020250329123038.png)

这是**相对熵（KL 散度）** 在模型拟合中的应用，即如何使用参数分布 $q(x|\theta)$ 来逼近未知分布 $p(x)$，并通过最小化 KL 散度来优化参数 $\theta$。

## 五、关于疑义度的Fano不等式

不等式：

![](image/Pasted%20image%2020250329133033.png)


$E$ 表示不等的情况，即 
$$E = 
\begin{cases} 
0, & \text{if } X = \hat{X} \\
1, & \text{if } X \neq \hat{X}
\end{cases}$$
$P_E$为出错的概率。

证明太复杂了，看不懂（

图解：

![](image/Pasted%20image%2020250329133523.png)


物理意义：

已知$\hat{X}$条件下，对$X$还存在的不确定性$H(X|\hat{X})$可分为两个部分。第一部分是估计$\hat{X}$是否准确，这部分的不确定性为$H(P_E)$；第二部分是如果估计是不准确的，这时$X$可能取值有$K-1$个，这部分的不确定性为$P_E \log(K-1)$。

$K=2$，表示两个事件，一个是正确，一个是错误。如果$P_E=1$，那么$H_2(P_E)=0$，也表示是完全确定，没有不确定性。

该定理是在证明香农信道编码定理的逆定理时必须使用的。

## 六、马尔可夫链

### 1. 定义

![](image/Pasted%20image%2020250329133809.png)

若随机变量序列$X_1,X_2,\dots,X_n$的联合概率分布可以写成如下形式：
$$p(x_1,x_2,\dots,x_n)=p(x_1)p(x_2|x_1)\cdots p(x_n|x_{n-1})$$
则称这$n$个随机变量构成马尔可夫链，记为：
$$X_1 \rightarrow X_2 \rightarrow \cdots \rightarrow X_n$$
物理意义上：
- 在已知它目前的状态（现在）的条件下，它未来的演变（将来）不依赖于它以往的演变（过去）
- 这种已知“现在”的条件下，“将来”与“过去”独立的特性称为马尔可夫性，具有这种性质的随机过程叫做马尔可夫过程。

### 2. 性质

1.$p(xyz)=p(x)p(y|x)p(z|y) \iff p(z|xy)=p(z|y)$
2.给定现在的状态，未来的状态和过去无关：

$p(xz|y)=\frac{p(xyz)}{p(y)}=\frac{p(x,y)p(z|y)}{p(y)}=p(x|y)p(z|y) \iff I(X;Z|Y)=0$

证明：
$$
\begin{aligned}
I(X;Z|Y)&=\sum_{x\in\mathcal{X}}\sum_{z\in\mathcal{Z}}\sum_{y\in\mathcal{Y}}p(x,y,z)\log\frac{p(x|y,z)}{q(x|y)}\\&=\sum_{x\in\mathcal{X}}\sum_{z\in\mathcal{Z}}\sum_{y\in\mathcal{Y}}p(x,y,z)\log\frac{p(x,z|y)}{q(x|y)w(z|y)}\\&=0
\end{aligned}
$$
3.如果 $X,Y,Z$ 构成马尔科夫链，那么 $Z,Y,X$ 也构成马尔科夫链。

![](image/Pasted%20image%2020250329135553.png)

## 七、数据处理定理

### 1. 定理

如果$X \rightarrow Y \rightarrow Z$, 那么

$I(X;Y) \geq I(X;Z)$
$I(X;Y) \geq I(X;Y|Z)$

证明：

$I(X;YZ)=I(X;Y)+I(X;Z|Y)=I(X;Z)+I(X;Y|Z)$ (1)

由于$X\rightarrow Y \rightarrow Z \Rightarrow I(X;Z|Y)=0, \ \ \therefore$
- $I(X;Y)\geq I(X;Z)$ (两者之差，$I(X;Y|Z)\geq 0$)
- $I(X;Y)\geq I(X;Y|Z)$ (同理，两者之差，$I(X;Z) \geq 0$)

由于$X\rightarrow Y \rightarrow Z$时，同时有$Z\rightarrow Y \rightarrow X$,于是有：

- $I(Y;Z)\geq I(X;Z)$ (原先我们可以得到$I(Z;Y)\geq I(Z;X)$, 利用一下互信息的对称性则可得到左式）
- $I(Y;Z)\geq I(Y;Z|X)$ (同理，$I(Z;Y)\geq I(Z;Y|X)$+对称）

注：
(1)是互信息的链式法则。详细的话可以用熵来证：
$I(X;Y)=H(X)-H(X|Y)$
$I(X;Z|Y)=H(X|Y)-H(X|Y,Z)$
上述两式相加：
$I(X;Y)+I(X;Z|Y)=H(X)-H(X|Y,Z)=I(X;YZ)$

右式同理。

## 八、四变量的马尔可夫链

![](image/Pasted%20image%2020250329173126.png)

定理：若$U \rightarrow X \rightarrow Y \rightarrow V$, 那么$I(X;Y) \geq I(U;V)$

证明：

我们引入中间变量 $I(Y;U)$, 即先证$I(X;Y) \geq I(U;Y) \geq I(U;V)$

由于$U \rightarrow X \rightarrow Y \iff Y \rightarrow X \rightarrow U$, 那么由**数据处理定理**，$I(Y;X)\geq I(Y;U)$
由$U \rightarrow Y \rightarrow V$,有$I(U;Y)\geq I(U;V)$, $\therefore I(X;Y)\geq I(U;V)$

Tips:
$U \rightarrow X \rightarrow Y \rightarrow V \Rightarrow U \rightarrow Y \rightarrow V$, 用定义可证，不过有点繁琐，也可以理解为$Y$和过去状态$X$无关。


## 九、互信息的凸性

+ 互信息的凸性

$I(X; Y) = \sum_x \sum_y p(xy) \log \frac{p(x|y)}{q(x)}$

$= \sum_x \sum_y q(x)p(y|x) \log \frac{p(y|x)}{\omega(y)}$

$= \sum_x \sum_y q(x)p(y|x) \log \frac{p(y|x)}{\sum_x q(x)p(y|x)}$

$= I(\{q(x)\}, \{p(y|x)\})$

- 互信息$I(X;Y)$是关于输入分布$\{q(x)\}$和转移概率矩阵$\{p(y|x)\}$的函数
- 定理1：当转移概率矩阵$\{p(y|x)\}$给定时，互信息$I(X;Y)=I(\{q(x)\})$是输入分布的上凸函数

![](image/Pasted%20image%2020250329200635.png)


- 定理2：当输入分布$\{q(x)\}$给定时，互信息$I(X;Y)=I(\{p(y|x)\})$是转移概率矩阵的下凸函数

![](image/Pasted%20image%2020250329200514.png)




