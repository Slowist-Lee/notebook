# 第四节、连续随机变量的互信息和微分熵

## 一、基本定义和性质

### 1. 引入 & 定义

在概统中，我们已经学了以下概念：

- $p_{XY}(x, y)$是连续随机变量$XY$的联合概率密度函数
- $p_X(x)$是$X$的边际概率密度
- $p_Y(y)$是$Y$的边际概率密度

则：

$p_X(x) = \int_{-\infty}^{+\infty} p_{XY}(x, y) dy$
$p_Y(y) = \int_{-\infty}^{+\infty} p_{XY}(x, y) dx$
$p_{Y|X}(y|x) = \frac{p_{XY}(x, y)}{p_X(x)}$

因为我们之前讨论的都是离散条件下的一些概率问题，所以我们到连续随机变量下就要进行微分，不过这部分和概统的思路是很类似的，看图解$\rightarrow$

![](image/Pasted%20image%2020250329203151.png)

因此，联合连续随机变量 $(X, Y), R^2, p_{XY}(x, y)$ 之间的互信息：

$I(X; Y) = \sum_{i=-\infty}^{+\infty} \sum_{j=-\infty}^{+\infty} [p_{XY}(x_i, y_j) \Delta x_i \Delta y_j] \log \frac{[p_{XY}(x_i, y_j) \Delta x_i \Delta y_j]}{[p_X(x_i) \Delta x_i][p_Y(y_j) \Delta y_j]}$

$= \sum_{i=-\infty}^{+\infty} \sum_{j=-\infty}^{+\infty} \left( p_{XY}(x_i, y_j) \log \frac{p_{XY}(x_i, y_j)}{p_X(x_i) p_Y(y_j)} \right) \Delta x_i \Delta y_j$

$\xrightarrow{\Delta x_i \to 0, \Delta y_j \to 0} \iint p_{XY}(x, y) \log \frac{p_{XY}(x, y)}{p_X(x) p_Y(y)} dx dy$
因此，也有类似的条件互信息，联合互信息的定义等，表达式如下：
$$
I(X; Y|Z) = \iiint p_{XYZ}(x, y, z) \log \frac{p_{XY|Z}(x, y|z)}{p_{X|Z}(x|z)p_{Y|Z}(y|z)} dx dy dz
$$

$$
I(X; YZ) = \iiint p_{XYZ}(x, y, z) \log \frac{p_{XYZ}(x, y, z)}{p_X(x)p_{YZ}(yz)} dx dy dz
$$

### 2. 性质

既然连续随机变量的推导是由离散化而来，其性质也是较为类似的。

- $I(X;Y) \geq 0$ 
- $I(X;Y)=I(Y;X),I(X;Y|Z)=I(Y;X|Z)$
- $I(X;YZ)=I(X;Y)+I(X;Z|Y)=I(X;Z)+I(X;Y|Z)$ （链式法则）
- （马尔可夫）若$X \rightarrow Y \rightarrow Z$, 则$I(X;Y) \leq I(X;Z);\ \ I(X;Y)\leq I(X;Y|Z)$ (原因：$I(X;Z|Y)=0$)

## 二、连续随机变量的微分熵

和离散随机变量类似的，我们尝试计算连续随机变量$(X,R,p_X(x))$的离散化熵值。回忆一下，之前我们在算$H(X)$时，所用的公式是$H(X)=-\sum_{k=1}^K p_k\log p_k$. 
在连续随机变量中，我们用$p_X(x_i)\Delta x_i$来代替对应的位置。

$H_{\Delta}(X) = -\sum_{i=-\infty}^{+\infty} p_X(x_i) \Delta x_i \log (p_X(x_i) \Delta x_i)$ , 

$\because \log(p_X(x_i) \Delta x_i)=\log(p_X(x_i))+ \log(\Delta x_i)$

$= -\sum_{i=-\infty}^{+\infty} [p_X(x_i) \log p_X(x_i)] \Delta x_i - \sum_{i=-\infty}^{+\infty} p_X(x_i) \Delta x_i \log \Delta x_i$

$\xrightarrow{\Delta x_i \to 0} -\int p_X(x) \log p_X(x) dx + \infty$
(右边求和大致类似$x\ln x -x +C$,会趋向无穷；左边化成微分的表达式，即$dx$)
由于我们计算得到的式子都是无穷，不好表示熵值的大小，于是我们定义了连续随机变量 $X$的微分熵：

$$
H_C(X) = h(X) \triangleq -\int p_X(x) \log p_X(x) dx
$$

因此，微分熵在一定程度上反映了该连续随机变量的相对不确定性，但并不是【真正的熵】。连续随机变量的不确定性一般都是无穷大。
由此，它的某些性质和熵也不太一样，$H_C(X)$ 可正，可负，可为 0。

例如：
$$
p(x) = 
\begin{cases} 
\frac{1}{b-a}, & x \in [a, b] \\
0, & x \notin [a, b]
\end{cases}
$$
$\because a,b \in \mathbf{C}, \Rightarrow H_C(X)=\int_a^b \frac{1}{b-a}\ln(b-a)dx=\ln(b-a)$

## 三、条件微分熵和联合熵及其性质

![](image/Pasted%20image%2020250329211819.png)

注意：**微分熵在线性变化下不具有不变性**。这是由【微分】的特性造成的。

对于离散随机变量 $X$，令 $Y = f(X)$ 是 $X \rightarrow Y$ 上的一对一函数，则 $H(X) = H(Y)$。

但对于连续随机变量：

$$
H_C(Y) = -\int p(y) \log p(y) dy = -\int p(x) \log p(x) f'(x) dx \neq H_C(X)
$$

即使对于线性变换，微分熵也不具有不变性。

例子：

![](image/Pasted%20image%2020250329205519.png)

![](image/Pasted%20image%2020250329211917.png)

计算互信息的例子：

![](image/Pasted%20image%2020250329212158.png)

计算过程：

![](image/Pasted%20image%2020250329212218.png)

（*抄PPT了，感觉连续变量的计算太繁琐了，~~懒得自己算一遍了~~*）

## 四、微分熵的极大化

### 1. 峰值受限

定理：设$X$取值限于$(-M,M)$, 即$\int_{-M}^M p(x)dx=1$,这时微分熵$H_C(X)\leq \ln 2M$ 等号在均匀分布时取到。

证明：
使用拉格朗日乘子法（微积分II）,约束条件即“$\int_{-M}^M p(x)dx=1$, 求$H_C(X)$的极大值：
令$J(p(x)) \triangleq H_C(X) - \lambda \int_{-M}^{+M}p(x)dx$， 则$J(p(x))$和$H_C(X)$应该是同时取到极值的。

因为无论是以2还是e为底，都能在常数这项里乘一个系数类似提出来，所以我们就以$\ln$简化运算了。

记$H_C(X)=-\int_{-M}^M p(x)\ln p(x)dx$
原式=$-\int_{-M}^M p(x)\ln p(x)dx-\lambda \int_{-M}^{+M} p(x)dx$
$=-\int^{+M}_{-M}p(x)(\ln p(x)+\lambda)dx$
$=-\int^{+M}_{-M}p(x)(\ln e^{\lambda}p(x))dx$  （$\ln x \leq x-1$）
$\leq \int^{+M}_{-M}p(x)(\frac{1}{e^{\lambda}p(x)}-1)dx$
$=\int^{+M}_{-M}\frac{1}{e^{\lambda}}dx-\int^{+M}_{-M}p(x)dx$
$=\frac{2M}{e^{\lambda}}-1=const$

（*这里的PPT有点问题（多了个负号），所以我重写了一遍*）

等号成立条件: $\frac{1}{e^\lambda p(x)}=1 \Rightarrow p(x)=e^{-\lambda}$ ，为均匀分布。
又$\because$$\int_{-M}^M p(x)dx=1$，$\therefore p(x)=\frac{1}{2M}$ 带入，$H_C(X)\leq \ln(2M)$


### 2. 平均功率受限

定理：在方差 $\sigma^2$ 一定条件下，当 $X$ 服从正态分布时，微分熵最大，即 $H_C(X) \leq \ln(\sqrt{2\pi e}\sigma)$。

证明：引入了方差$\sigma^2$一定，即多了一个约束条件$\lambda_2\int_{-\infty}^{\infty}p(x)(x-m)^2$, 其他证明思路类似，我也没细算过。

![](image/Pasted%20image%2020250329214302.png)

**意义：** 通信往往假设噪声为高斯的；这里提供了理论依据。高斯是最混乱的，所以**高斯是最难解决的**。若高斯都解决了，一切都能解决。

## 五、熵功率

我对这部分的理解是之前的书写形式过于复杂了，只是定义一种更便于讨论的$\sigma^2$来表示平均功率受限定理，没有别的发现。
### 1. 定义：

$$
\overline{\sigma_x^2} = \frac{1}{2\pi e} e^{2H_C(X)}
$$
（由平均功率受限推导出来的。我们刚刚有 $H_C(X) \leq \ln(\sqrt{2\pi e}\sigma) \Rightarrow e^{H_C(X)} \leq \sqrt{2\pi e}\sigma \Rightarrow e^{2H_C(X)} \leq 2\pi e\sigma^2$)

### 2. 高斯随机变量的熵功率

高斯随机变量 $X \sim p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left\{ -\frac{1}{2\sigma^2} x^2 \right\}$ 的微分熵为

$$
H_C(X) = \frac{1}{2} \ln (2\pi e \sigma^2)
$$

其熵功率为

$$
\overline{\sigma_x^2} = \frac{1}{2\pi e} e^{2H_C(X)} = \sigma^2
$$

刚好为高斯随机变量的方差。

### 3. 熵功率不等式

$H_C(X) \leq \ln(\sqrt{2\pi e}\sigma)  \iff \overline{\sigma_x^2} = \frac{1}{2\pi e} e^{2H_C(X)} \leq \sigma^2$

Tips:

1. 随机变量的熵功率一般不大于功率，只有高斯变量其熵功率与功率相等。
2. 功率一定时，高斯变量的熵功率最大，与功率相等。

## 六、小结

![](image/Pasted%20image%2020250329215459.png)

